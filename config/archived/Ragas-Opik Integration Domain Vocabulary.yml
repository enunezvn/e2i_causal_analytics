title: "Ragas-Opik Integration Domain Vocabulary"
version: "1.0.0"
last_updated: "2024-12-22"
purpose: "Standardize terminology for agent evaluation and observability"

core_concepts:
  fundamental_distinction:
    - term: "THE WHAT"
      definition: "Quantitative assessment of agent performance via metrics"
      role: "Ragas provides this"
    - term: "THE WHY"
      definition: "Qualitative understanding of agent behavior via traces"
      role: "Opik provides this"

ragas_domain_terms:
  evaluation_concepts:
    - term: "Metric"
      definition: "A quantifiable measure of a specific quality aspect"
      example: "faithfulness, answer_relevancy"
    - term: "Score"
      definition: "Numeric result of a metric computation (0.0 - 1.0)"
      example: "faithfulness_score = 0.85"
    - term: "Ground Truth"
      definition: "The expected/correct answer for reference-based evaluation"
      example: "Human-annotated answer"
    - term: "Sample"
      definition: "A single evaluation unit containing question, answer, contexts"
      example: "One Q&A pair with retrieved docs"
    - term: "Dataset"
      definition: "Collection of samples for batch evaluation"
      example: "Golden test set"
    - term: "Threshold"
      definition: "Minimum acceptable score for a metric"
      example: "faithfulness >= 0.7"

  metric_categories:
    - category: "Retrieval Metrics"
      definition: "Evaluate quality of document retrieval"
      metrics_included: "context_precision, context_recall, context_entity_recall"
    - category: "Generation Metrics"
      definition: "Evaluate quality of LLM-generated responses"
      metrics_included: "faithfulness, answer_relevancy, answer_similarity"
    - category: "Combined Metrics"
      definition: "Evaluate overall system performance"
      metrics_included: "answer_correctness, summarization_score"

  specific_metrics:
    - metric: "faithfulness"
      definition: "Degree to which answer is grounded in provided context"
      measures: "Hallucination prevention"
    - metric: "answer_relevancy"
      definition: "How well the answer addresses the original question"
      measures: "Response appropriateness"
    - metric: "context_precision"
      definition: "Whether relevant items rank higher than irrelevant ones"
      measures: "Retrieval ranking quality"
    - metric: "context_recall"
      definition: "Coverage of ground truth information in retrieved context"
      measures: "Retrieval completeness"
    - metric: "context_entity_recall"
      definition: "Presence of key entities from ground truth in context"
      measures: "Entity coverage"
    - metric: "answer_similarity"
      definition: "Semantic similarity between answer and ground truth"
      measures: "Response accuracy"
    - metric: "answer_correctness"
      definition: "Overall correctness combining similarity and factual accuracy"
      measures: "Holistic quality"

  data_fields:
    - field: "question"
      definition: "User's input query (also: user_input)"
      used_by: "All metrics"
    - field: "answer"
      definition: "Agent's generated response (also: response)"
      used_by: "Generation metrics"
    - field: "contexts"
      definition: "List of retrieved document texts (also: retrieved_contexts)"
      used_by: "All RAG metrics"
    - field: "ground_truth"
      definition: "Expected correct answer (also: reference)"
      used_by: "Reference-based metrics"

opik_domain_terms:
  tracing_concepts:
    - term: "Trace"
      definition: "Complete record of a single agent invocation"
      example: "One user query → response cycle"
    - term: "Span"
      definition: "Individual operation within a trace"
      example: "Retrieval step, LLM call"
    - term: "Parent Span"
      definition: "Span that contains child spans"
      example: "Agent orchestration span"
    - term: "Child Span"
      definition: "Span nested within a parent"
      example: "Individual tool call"
    - term: "Trace ID"
      definition: "Unique identifier for a trace"
      example: "trace_abc123xyz"
    - term: "Span ID"
      definition: "Unique identifier for a span"
      example: "span_def456uvw"

  trace_attributes:
    - attribute: "Input"
      definition: "Data sent to the traced operation"
      example: '{"question": "What is ML?"}'
    - attribute: "Output"
      definition: "Data returned from the traced operation"
      example: '{"answer": "ML is..."}'
    - attribute: "Metadata"
      definition: "Additional contextual information"
      example: '{"model": "gpt-4", "temperature": 0.1}'
    - attribute: "Tags"
      definition: "Labels for filtering and organization"
      example: '["production", "rag", "v2"]'
    - attribute: "Duration"
      definition: "Time taken for operation (milliseconds)"
      example: "1250"
    - attribute: "Timestamp"
      definition: "When the operation occurred"
      example: "2024-01-15T10:30:00Z"

  feedback_and_scoring:
    - term: "Feedback Score"
      definition: "Numeric evaluation attached to trace/span"
      example: '{"name": "faithfulness", "value": 0.85}'
    - term: "Annotation"
      definition: "Human-provided feedback on a trace"
      example: "Thumbs up/down, quality rating"
    - term: "Category"
      definition: "Grouping for feedback scores"
      example: "ragas_evaluation, human_review"

  observability_concepts:
    - term: "Project"
      definition: "Container for related traces"
      purpose: "Organize by application/environment"
    - term: "Workspace"
      definition: "Top-level organizational unit"
      purpose: "Team/organization isolation"
    - term: "Experiment"
      definition: "Named collection of evaluation runs"
      purpose: "A/B testing, version comparison"
    - term: "Dashboard"
      definition: "Visual aggregation of trace metrics"
      purpose: "Monitoring and reporting"
    - term: "Alert"
      definition: "Notification triggered by metric conditions"
      purpose: "Performance degradation warning"

  integration_points:
    - term: "@track Decorator"
      definition: "Python decorator to auto-trace functions"
      usage: '@track(name="my_agent")'
    - term: "OpikTracer"
      definition: "Callback for framework integrations"
      usage: "LangChain, Ragas callbacks"
    - term: "track_openai"
      definition: "Wrapper for OpenAI client tracing"
      usage: "Token usage capture"

integration_terms:
  combined_concepts:
    - term: "Evaluation Report"
      definition: "Complete assessment combining scores and traces"
      components: "Ragas scores + Opik trace data"
    - term: "Root Cause Analysis"
      definition: "Process of using traces to explain low scores"
      components: "Score → Trace → Issue identification"
    - term: "Issue Detection"
      definition: "Automated identification of performance problems"
      components: "Threshold violations"
    - term: "Recommendation"
      definition: "Suggested fix based on analysis"
      components: '"Improve retrieval k parameter"'

  workflow_terms:
    - term: "Online Evaluation"
      definition: "Real-time scoring of production traffic"
      stage: "Runtime"
    - term: "Offline Evaluation"
      definition: "Batch evaluation of test datasets"
      stage: "Development/CI"
    - term: "Sampling"
      definition: "Evaluating subset of traffic for efficiency"
      stage: "Production monitoring"
    - term: "Golden Set"
      definition: "Curated test cases with ground truth"
      stage: "Regression testing"

  score_interpretation:
    - score_range: "0.8 - 1.0"
      interpretation: "Excellent"
      action: "No action needed"
    - score_range: "0.7 - 0.8"
      interpretation: "Good"
      action: "Monitor for trends"
    - score_range: "0.5 - 0.7"
      interpretation: "Warning"
      action: "Investigate and improve"
    - score_range: "0.0 - 0.5"
      interpretation: "Critical"
      action: "Immediate attention required"

rag_pipeline_terms:
  components:
    - term: "Retriever"
      definition: "Component that fetches relevant documents"
      traced_as: "retrieval span"
    - term: "Generator"
      definition: "LLM that produces responses"
      traced_as: "generation span"
    - term: "Reranker"
      definition: "Model that reorders retrieved documents"
      traced_as: "reranking span"
    - term: "Embedder"
      definition: "Model that creates vector representations"
      traced_as: "embedding span"

  data_artifacts:
    - term: "Query Embedding"
      definition: "Vector representation of user question"
      format: "float[768] or float[1536]"
    - term: "Document Embedding"
      definition: "Vector representation of document chunk"
      format: "float[768] or float[1536]"
    - term: "Similarity Score"
      definition: "Cosine/dot product between embeddings"
      format: "0.0 - 1.0"
    - term: "Context Window"
      definition: "Text provided to LLM for generation"
      format: "Concatenated retrieved docs"
    - term: "Prompt"
      definition: "Full input to LLM including instructions"
      format: "System + context + question"

abbreviations:
  - abbrev: "RAG"
    full_term: "Retrieval-Augmented Generation"
    domain: "Architecture"
  - abbrev: "LLM"
    full_term: "Large Language Model"
    domain: "AI/ML"
  - abbrev: "QA"
    full_term: "Question Answering"
    domain: "Task type"
  - abbrev: "GT"
    full_term: "Ground Truth"
    domain: "Evaluation"
  - abbrev: "SDK"
    full_term: "Software Development Kit"
    domain: "Integration"
  - abbrev: "API"
    full_term: "Application Programming Interface"
    domain: "Integration"

naming_conventions:
  python_code: |
    # Metrics: snake_case
    faithfulness_score = 0.85
    context_precision = compute_context_precision()

    # Classes: PascalCase
    class RagasMetricWrapper:
    class CombinedEvaluator:

    # Functions: snake_case with verb prefix
    def evaluate_single()
    def get_traces_by_experiment()
    def log_ragas_scores()

    # Constants: UPPER_SNAKE_CASE
    DEFAULT_THRESHOLD = 0.7
    MAX_CONTEXTS = 10
  configuration: |
    # Project names: kebab-case
    project_name: ragas-opik-evaluation

    # Metric names: snake_case
    metrics:
      - faithfulness
      - answer_relevancy
      - context_precision

    # Tags: kebab-case
    tags:
      - production
      - experiment-v1
      - rag-pipeline
  opik_traces: |
    # Trace names: snake_case describing operation
    rag_agent_query
    retrieval_step
    llm_generation

    # Span hierarchy
    rag_agent_query (parent trace)
    ├── retrieval (span)
    ├── reranking (span)
    └── generation (span)

relationships:
  evaluation_flow: |
    ┌─────────────────────────────────────────────────────────────────┐
    │                       EVALUATION FLOW                           │
    ├─────────────────────────────────────────────────────────────────┤
    │                                                                 │
    │  Sample ──contains──> question, answer, contexts, ground_truth  │
    │     │                                                           │
    │     └──evaluated_by──> Metric ──produces──> Score               │
    │                           │                                     │
    │                           └──logged_to──> Trace (via Opik)      │
    │                                              │                  │
    │                                              ├──> Span (retrieval)
    │                                              ├──> Span (generation)
    │                                              └──> Feedback Scores
    │                                                                 │
    └─────────────────────────────────────────────────────────────────┘
  debugging_flow: |
    ┌─────────────────────────────────────────────────────────────────┐
    │                      DEBUGGING FLOW                             │
    ├─────────────────────────────────────────────────────────────────┤
    │                                                                 │
    │  Low Score (Ragas) ──triggers──> Investigation                  │
    │       │                              │                          │
    │       └──────────────────────────────┼──> Trace Inspection (Opik)
    │                                      │        │                 │
    │                                      │        ├──> Span Details │
    │                                      │        ├──> Inputs/Outputs
    │                                      │        └──> Errors/Latency
    │                                      │                          │
    │                                      └──> Root Cause Identified │
    │                                                  │              │
    │                                                  └──> Recommendation
    │                                                                 │
    └─────────────────────────────────────────────────────────────────┘

version_history:
  - version: "1.0.0"
    date: "2024-12-22"
    changes: "Initial vocabulary definition"

references:
  - title: "Ragas Documentation"
    url: "https://docs.ragas.io/"
  - title: "Opik Documentation"
    url: "https://www.comet.com/docs/opik/"
  - title: "Ragas-Opik Integration Guide"
    url: "./ragas_opik_integration.html"