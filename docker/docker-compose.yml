# =============================================================================
# E2I Causal Analytics - Production Docker Compose
# =============================================================================
# 18-Agent 6-Tier Architecture | Tri-Memory System | MLOps Integration
#
# Usage:
#   Production:  docker compose -f docker/docker-compose.yml up -d
#   With debug:  docker compose -f docker/docker-compose.yml --profile debug up -d
#
# External Dependencies:
#   - Supabase Cloud (PostgreSQL + pgvector)
#   - Anthropic API (Claude for agents)
# =============================================================================

name: e2i-causal-analytics

# =============================================================================
# EXTENSION FIELDS (Shared configuration)
# =============================================================================

x-logging: &default-logging
  driver: json-file
  options:
    max-size: "10m"
    max-file: "3"

x-common-env: &common-env
  SUPABASE_URL: ${SUPABASE_URL}
  SUPABASE_KEY: ${SUPABASE_KEY}
  SUPABASE_SERVICE_KEY: ${SUPABASE_SERVICE_KEY}
  REDIS_URL: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/0
  FALKORDB_URL: redis://:${FALKORDB_PASSWORD:?FALKORDB_PASSWORD must be set}@falkordb:6379/0
  MLFLOW_TRACKING_URI: http://mlflow:5000
  BENTOML_URL: http://bentoml:3000
  FEAST_URL: http://feast:6566
  OPIK_URL: http://opik:5174
  CELERY_BROKER_URL: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/1
  CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/2
  ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
  ENVIRONMENT: production
  LOG_LEVEL: INFO

x-common-worker: &common-worker
  build:
    context: ..
    dockerfile: docker/Dockerfile
    target: production
  restart: unless-stopped
  security_opt:
    - no-new-privileges:true
  networks:
    - e2i_network
    - mlops_network

# =============================================================================
# VOLUMES
# =============================================================================
volumes:
  # Persistent volumes (state - survives container restarts)
  redis_data:
    driver: local
    name: e2i_redis_data
  falkordb_data:
    driver: local
    name: e2i_falkordb_data
  mlflow_db:
    driver: local
    name: e2i_mlflow_db
  mlflow_artifacts:
    driver: local
    name: e2i_mlflow_artifacts
  opik_data:
    driver: local
    name: e2i_opik_data
  bentoml_models:
    driver: local
    name: e2i_bentoml_models

  # Shared volumes (inter-container data exchange)
  ml_artifacts:
    driver: local
    name: e2i_ml_artifacts
  model_registry:
    driver: local
    name: e2i_model_registry
  causal_outputs:
    driver: local
    name: e2i_causal_outputs
  feature_cache:
    driver: local
    name: e2i_feature_cache
  agent_outputs:
    driver: local
    name: e2i_agent_outputs
  experiment_designs:
    driver: local
    name: e2i_experiment_designs

  # Observability volumes
  prometheus_data:
    driver: local
    name: e2i_prometheus_data
  loki_data:
    driver: local
    name: e2i_loki_data
  grafana_data:
    driver: local
    name: e2i_grafana_data

# =============================================================================
# NETWORKS
# =============================================================================
networks:
  e2i_network:
    driver: bridge
    name: e2i_network
  # Isolated network for MLOps services
  mlops_network:
    driver: bridge
    name: e2i_mlops_network

# =============================================================================
# SERVICES
# =============================================================================
services:
  # ===========================================================================
  # TIER 0: CONFIGURATION VALIDATION
  # ===========================================================================

  config-check:
    image: alpine:3.19
    container_name: e2i_config_check
    entrypoint: /bin/sh
    command:
      - -c
      - |
        FAIL=0
        for VAR_NAME in REDIS_PASSWORD FALKORDB_PASSWORD; do
          eval VAL="\$$VAR_NAME"
          if [ "$$VAL" = "changeme" ] || [ -z "$$VAL" ]; then
            if [ "$$ENVIRONMENT" = "production" ]; then
              echo "ERROR: $$VAR_NAME must be set to a secure value in production"
              FAIL=1
            else
              echo "WARNING: $$VAR_NAME is unset or default - acceptable for development only"
            fi
          fi
        done
        if [ "$$FAIL" = "1" ]; then exit 1; fi
        echo "Configuration validation passed"
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD:?REDIS_PASSWORD must be set}
      FALKORDB_PASSWORD: ${FALKORDB_PASSWORD:?FALKORDB_PASSWORD must be set}
      ENVIRONMENT: ${ENVIRONMENT:-development}
    restart: "no"
    networks: []

  # ===========================================================================
  # TIER 1: INFRASTRUCTURE (Memory Systems)
  # ===========================================================================

  # Redis - Working Memory + LangGraph Checkpointing + Celery Broker
  redis:
    image: redis:7-alpine
    container_name: e2i_redis
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "6382:6379"  # 6382 external to avoid conflict with host Redis on 6379
    volumes:
      - redis_data:/data
    command: >
      redis-server
      --appendonly yes
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --requirepass ${REDIS_PASSWORD:?REDIS_PASSWORD must be set}
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - e2i_network
    security_opt:
      - no-new-privileges:true
    depends_on:
      config-check:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    labels:
      - "e2i.service=infrastructure"
      - "e2i.memory=working"

  # FalkorDB - Semantic Memory (Graphity Auto-Extraction)
  falkordb:
    image: falkordb/falkordb:v4.14.11
    container_name: e2i_falkordb
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "6381:6379"  # 6381 external to avoid conflict with Redis
    volumes:
      - falkordb_data:/data
    environment:
      - FALKORDB_ARGS=--maxmemory 1gb --requirepass ${FALKORDB_PASSWORD:?FALKORDB_PASSWORD must be set}
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "6379", "-a", "${FALKORDB_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    networks:
      - e2i_network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    labels:
      - "e2i.service=infrastructure"
      - "e2i.memory=semantic"

  # FalkorDB Browser - Interactive Graph Visualization (debug only)
  falkordb-browser:
    image: falkordb/falkordb-browser:v1.7.1
    container_name: e2i_falkordb_browser
    restart: unless-stopped
    logging: *default-logging
    profiles:
      - debug
    ports:
      - "127.0.0.1:3030:3000"
    environment:
      - FALKORDB_HOST=falkordb
      - FALKORDB_PORT=6379
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    networks:
      - e2i_network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    depends_on:
      falkordb:
        condition: service_healthy
    labels:
      - "e2i.service=infrastructure"
      - "e2i.memory=semantic-browser"

  # ===========================================================================
  # TIER 2: MLOPS SERVICES
  # ===========================================================================

  # MLflow - Experiment Tracking & Model Registry
  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.1.0
    container_name: e2i_mlflow
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "127.0.0.1:5000:5000"
    volumes:
      - mlflow_db:/mlflow/db
      - mlflow_artifacts:/mlflow/artifacts
      - ml_artifacts:/mlflow/ml_artifacts      # Shared: receives from training
      - model_registry:/mlflow/model_registry  # Shared: sends to BentoML
    environment:
      - MLFLOW_TRACKING_URI=sqlite:///mlflow/db/mlflow.db
      - MLFLOW_ARTIFACT_ROOT=/mlflow/artifacts
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow/db/mlflow.db
      --default-artifact-root mlflow-artifacts:
      --serve-artifacts
      --artifacts-destination /mlflow/artifacts
      --host 0.0.0.0
      --port 5000
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5000/')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - e2i_network
      - mlops_network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    labels:
      - "e2i.service=mlops"
      - "e2i.component=experiment-tracking"

  # BentoML - Model Serving (Decoupled from Agents)
  bentoml:
    image: bentoml/bento-server:1.4.0
    container_name: e2i_bentoml
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "127.0.0.1:3000:3000"
    volumes:
      - bentoml_models:/home/bentoml/bentos
      - model_registry:/models:ro  # Read-only: receives from MLflow
    environment:
      - BENTOML_HOME=/home/bentoml
      - BENTOML_PORT=3000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - e2i_network
      - mlops_network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    depends_on:
      mlflow:
        condition: service_healthy
    labels:
      - "e2i.service=mlops"
      - "e2i.component=model-serving"

  # Feast - Feature Store (Online + Offline Serving)
  feast:
    build:
      context: .
      dockerfile: Dockerfile.feast
    container_name: e2i_feast
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "6566:6566"
    volumes:
      - feature_cache:/feast/data  # Shared: feature materialization
    environment:
      - FEAST_OFFLINE_STORE_TYPE=file
      - FEAST_ONLINE_STORE_TYPE=redis
      - FEAST_REDIS_HOST=redis
      - FEAST_REDIS_PORT=6379
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6566/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - e2i_network
      - mlops_network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    depends_on:
      redis:
        condition: service_healthy
    labels:
      - "e2i.service=mlops"
      - "e2i.component=feature-store"

  # Opik - LLM & Agent Observability (Self-Hosted)
  opik:
    image: comet-ml/opik:1.10.2
    container_name: e2i_opik
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "127.0.0.1:5173:5173"  # UI
      - "127.0.0.1:5174:5174"  # API
    volumes:
      - opik_data:/app/data
    environment:
      - OPIK_BASE_URL=http://localhost:5173
      - OPIK_BACKEND_URL=http://localhost:5174
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5173/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    networks:
      - e2i_network
      - mlops_network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    labels:
      - "e2i.service=mlops"
      - "e2i.component=observability"

  # ===========================================================================
  # TIER 3: APPLICATION SERVICES
  # ===========================================================================

  # FastAPI Backend - 18-Agent Orchestration Layer
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: production
    container_name: e2i_api
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "8000:8000"
    volumes:
      - ml_artifacts:/app/data/ml_artifacts
      - causal_outputs:/app/data/causal_outputs
      - agent_outputs:/app/data/agent_outputs
      - experiment_designs:/app/data/experiment_designs
      - feature_cache:/app/data/feature_cache:ro  # Read from Feast
    environment:
      <<: *common-env
      CORS_ORIGINS: ${CORS_ORIGINS:-http://localhost:3002}
      WORKERS: 4
    command: >
      gunicorn src.api.main:app
      --workers 4
      --worker-class uvicorn.workers.UvicornWorker
      --bind 0.0.0.0:8000
      --timeout 120
      --graceful-timeout 30
      --keep-alive 5
      --max-requests 1000
      --max-requests-jitter 50
      --access-logfile -
      --error-logfile -
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - e2i_network
      - mlops_network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp:size=50M,mode=1777
      - /app/tmp:size=100M,mode=1777
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 5G
        reservations:
          cpus: '1'
          memory: 2G
    depends_on:
      redis:
        condition: service_healthy
      falkordb:
        condition: service_healthy
      mlflow:
        condition: service_healthy
    labels:
      - "e2i.service=application"
      - "e2i.component=api"

  # ---------------------------------------------------------------------------
  # Celery Workers - Multi-Tier Architecture
  # ---------------------------------------------------------------------------

  # Worker Light - Quick tasks (API calls, cache updates, notifications)
  worker_light:
    <<: *common-worker
    logging: *default-logging
    volumes:
      - ml_artifacts:/app/data/ml_artifacts
      - causal_outputs:/app/data/causal_outputs
      - agent_outputs:/app/data/agent_outputs
      - experiment_designs:/app/data/experiment_designs
      - feature_cache:/app/data/feature_cache:ro
    tmpfs:
      - /app/tmp:size=256M,mode=1770
    environment:
      <<: *common-env
      WORKER_TYPE: light
    command: >
      celery -A src.workers.celery_app worker
      --loglevel=INFO
      --concurrency=4
      --queues=default,quick,api
      --prefetch-multiplier=2
      --max-tasks-per-child=100
      --hostname=worker_light@%h
    healthcheck:
      test: ["CMD", "celery", "-A", "src.workers.celery_app", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
      replicas: 2  # Always run 2 light workers
    depends_on:
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    labels:
      - "e2i.service=application"
      - "e2i.component=worker"
      - "e2i.worker_tier=light"

  # Worker Medium - Standard analytics (reports, aggregations, dashboards)
  worker_medium:
    <<: *common-worker
    logging: *default-logging
    volumes:
      - ml_artifacts:/app/data/ml_artifacts
      - causal_outputs:/app/data/causal_outputs
      - agent_outputs:/app/data/agent_outputs
      - experiment_designs:/app/data/experiment_designs
      - feature_cache:/app/data/feature_cache:ro
    tmpfs:
      - /app/tmp:size=2G,mode=1770
    environment:
      <<: *common-env
      WORKER_TYPE: medium
    command: >
      celery -A src.workers.celery_app worker
      --loglevel=INFO
      --concurrency=2
      --queues=analytics,reports,aggregations
      --prefetch-multiplier=1
      --max-tasks-per-child=50
      --hostname=worker_medium@%h
    healthcheck:
      test: ["CMD", "celery", "-A", "src.workers.celery_app", "inspect", "ping"]
      interval: 45s
      timeout: 15s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
      replicas: 1  # Start with 1, can scale 0-3
    depends_on:
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    labels:
      - "e2i.service=application"
      - "e2i.component=worker"
      - "e2i.worker_tier=medium"

  # Worker Heavy - Compute-intensive tasks (SHAP, causal refutation, ML, twins)
  worker_heavy:
    <<: *common-worker
    logging: *default-logging
    volumes:
      - ml_artifacts:/app/data/ml_artifacts
      - causal_outputs:/app/data/causal_outputs
      - agent_outputs:/app/data/agent_outputs
      - experiment_designs:/app/data/experiment_designs
      - feature_cache:/app/data/feature_cache:ro
    tmpfs:
      - /app/tmp:size=8G,mode=1770  # Large scratch space for heavy computation
    environment:
      <<: *common-env
      WORKER_TYPE: heavy
      # Performance tuning for heavy tasks
      OMP_NUM_THREADS: 16  # OpenMP threads for numpy/scipy
      OPENBLAS_NUM_THREADS: 16  # BLAS parallelization
      MKL_NUM_THREADS: 16  # Intel MKL if available
    command: >
      celery -A src.workers.celery_app worker
      --loglevel=INFO
      --concurrency=2
      --queues=shap,causal,ml,twins
      --prefetch-multiplier=1
      --max-tasks-per-child=10
      --time-limit=3600
      --soft-time-limit=3300
      --hostname=worker_heavy@%h
    healthcheck:
      test: ["CMD", "celery", "-A", "src.workers.celery_app", "inspect", "ping"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '16'
          memory: 32G
        reservations:
          cpus: '8'
          memory: 16G
      replicas: 1  # Start with 1, scale on-demand (1-4)
    depends_on:
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    labels:
      - "e2i.service=application"
      - "e2i.component=worker"
      - "e2i.worker_tier=heavy"
      - "e2i.autoscale=true"

  # Celery Beat - Scheduled Tasks (Drift Monitoring, Health Checks)
  scheduler:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: production
    container_name: e2i_scheduler
    restart: unless-stopped
    logging: *default-logging
    volumes:
      - causal_outputs:/app/data/causal_outputs:ro  # Read-only for monitoring
    environment:
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/1
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD must be set}@redis:6379/2
      SUPABASE_URL: ${SUPABASE_URL}
      SUPABASE_KEY: ${SUPABASE_KEY}
    command: >
      celery -A src.workers.celery_app beat
      --loglevel=INFO
      --scheduler=celery.beat:PersistentScheduler
    healthcheck:
      test: ["CMD-SHELL", "celery -A src.workers.celery_app inspect ping --timeout 10 2>/dev/null | grep -q pong || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 30s
    networks:
      - e2i_network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp:size=50M,mode=1770
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    depends_on:
      redis:
        condition: service_healthy
      worker_light:
        condition: service_healthy
    labels:
      - "e2i.service=application"
      - "e2i.component=scheduler"

  # Frontend - React Dashboard (Production Build)
  # Note: Vite env vars (VITE_*) are baked at build time, not runtime.
  # The frontend defaults to /api for API requests (see frontend/src/config/env.ts),
  # which is correct when served behind nginx.
  frontend:
    build:
      context: ..
      dockerfile: docker/frontend/Dockerfile
      target: production
    container_name: e2i_frontend
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "3002:80"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    networks:
      - e2i_network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /var/cache/nginx:size=10M,mode=0770
      - /var/run:size=5M,mode=0770
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    depends_on:
      api:
        condition: service_healthy
    labels:
      - "e2i.service=application"
      - "e2i.component=frontend"

  # ===========================================================================
  # TIER 5: OBSERVABILITY STACK
  # ===========================================================================

  # Prometheus - Metrics Collection & Alerting
  prometheus:
    image: prom/prometheus:v3.2.1
    container_name: e2i_prometheus
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "127.0.0.1:9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    networks:
      - e2i_network
      - mlops_network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp:size=10M,mode=1770
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    labels:
      - "e2i.service=observability"
      - "e2i.component=prometheus"

  # Alertmanager - Alert Routing & Notification
  alertmanager:
    image: prom/alertmanager:v0.28.1
    container_name: e2i_alertmanager
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "127.0.0.1:9093:9093"
    volumes:
      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    networks:
      - e2i_network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "e2i.service=observability"
      - "e2i.component=alertmanager"

  # Node Exporter - Host System Metrics
  node-exporter:
    image: prom/node-exporter:v1.9.0
    container_name: e2i_node_exporter
    restart: unless-stopped
    logging: *default-logging
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - e2i_network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "e2i.service=observability"
      - "e2i.component=node-exporter"

  # Postgres Exporter - Supabase Database Metrics
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.16.0
    container_name: e2i_postgres_exporter
    restart: unless-stopped
    logging: *default-logging
    environment:
      DATA_SOURCE_NAME: ${SUPABASE_DB_URL:?SUPABASE_DB_URL must be set for postgres-exporter}
    networks:
      - e2i_network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "e2i.service=observability"
      - "e2i.component=postgres-exporter"

  # Loki - Log Aggregation
  loki:
    image: grafana/loki:3.4.2
    container_name: e2i_loki
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "127.0.0.1:3101:3100"
    volumes:
      - ./loki/loki-config.yml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - e2i_network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    labels:
      - "e2i.service=observability"
      - "e2i.component=loki"

  # Promtail - Log Collector (ships to Loki)
  promtail:
    image: grafana/promtail:3.4.2
    container_name: e2i_promtail
    restart: unless-stopped
    logging: *default-logging
    volumes:
      - ./promtail/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - e2i_network
    security_opt:
      - no-new-privileges:true
    depends_on:
      loki:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "e2i.service=observability"
      - "e2i.component=promtail"

  # Grafana - Dashboards & Visualization
  grafana:
    image: grafana/grafana:11.5.2
    container_name: e2i_grafana
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "127.0.0.1:3200:3000"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:?GRAFANA_ADMIN_PASSWORD must be set}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3200
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - e2i_network
      - mlops_network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp:size=10M,mode=1770
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    labels:
      - "e2i.service=observability"
      - "e2i.component=grafana"
