# E2I Causal Analytics v4.2.0 - Implementation TODO List

**Version:** 4.2.0
**Created:** 2024-12-17
**Status:** Database migrations complete, application integration pending

---

## Overview

This document tracks the remaining implementation tasks to fully activate the Digital Twin Engine and Tool Composer features in the E2I Causal Analytics platform. All database migrations (012 and 013) have been successfully applied to Supabase.

**Current Status:**
- ✅ Database schema (9 tables, 4 views, 4 ENUMs)
- ✅ Python module structure integrated
- ✅ Tool registry seeded (13 tools, 10 dependencies)
- ✅ Documentation updated
- ⏳ Application integration (pending)
- ⏳ LLM integration (pending)
- ⏳ API endpoints (pending)
- ⏳ Testing (pending)

---

## Prerequisites

Before implementing the tasks below, ensure you have:

- [ ] OpenAI or Anthropic API key configured in environment
- [ ] Supabase connection string in `.env`
- [ ] MLflow server running (for Digital Twin experiments)
- [ ] Python dependencies installed from `requirements.txt`
- [ ] Development database with test data loaded

---

## Section 1: Digital Twin ML Algorithms

**Status:** Python modules integrated, database ready, ML pipeline pending
**Priority:** High
**Depends on:** MLflow server, Supabase connection, test data

### 1.1 Twin Generator Integration

**File:** `src/digital_twin/twin_generator.py`

**Tasks:**
- [ ] Configure MLflow tracking URI in environment
- [ ] Implement `train_propensity_model()` with scikit-learn/XGBoost
  - Connect to HCP/patient data tables
  - Feature engineering for propensity matching
  - Model training with cross-validation
  - MLflow experiment logging
- [ ] Implement `generate_twin()` matching algorithm
  - Propensity score matching
  - Covariate balance checking
  - Twin pair validation
- [ ] Add database persistence via `twin_repository.py`
  - Save twins to `digital_twin_models` table
  - Log MLflow model URIs
- [ ] Add error handling and logging

**Testing Requirements:**
- Sample HCP/patient dataset (minimum 1000 records)
- Propensity model accuracy > 0.75
- Twin match quality metrics (SMD < 0.1)

---

### 1.2 Simulation Engine Integration

**File:** `src/digital_twin/simulation_engine.py`

**Tasks:**
- [ ] Implement `run_intervention_simulation()`
  - Load twin from database by `twin_id`
  - Apply intervention parameters (channel, frequency, content)
  - Generate simulated outcome using twin model
  - Calculate confidence intervals
- [ ] Implement `compare_to_baseline()`
  - Fetch baseline metrics
  - Calculate lift/delta
  - Statistical significance testing
- [ ] Add result persistence
  - Save to `twin_simulations` table
  - Link to parent twin model
  - Store recommendation (deploy/skip/refine)
- [ ] Generate simulation visualizations (optional)

**Testing Requirements:**
- End-to-end simulation test with known intervention
- Validate recommendation logic (threshold tuning)
- Performance: simulation latency < 2 seconds

---

### 1.3 Fidelity Tracker Integration

**File:** `src/digital_twin/fidelity_tracker.py`

**Tasks:**
- [ ] Implement `calculate_distribution_similarity()`
  - Compare twin vs real population distributions
  - Use KS-test, Chi-square, PSI metrics
  - Calculate aggregate fidelity score
- [ ] Implement `track_prediction_accuracy()`
  - Compare predicted outcomes to actual A/B test results
  - Calculate MAPE, RMSE
  - Update fidelity grade (excellent/good/fair/poor)
- [ ] Add automated fidelity monitoring
  - Periodic validation jobs
  - Alert when fidelity degrades
- [ ] Persist to `twin_fidelity_tracking` table

**Testing Requirements:**
- Fidelity calculation on synthetic twin/population
- Accuracy tracking with historical A/B test data

---

### 1.4 Tool Integration

**Files:**
- `src/agents/experiment_designer/tools/simulate_intervention_tool.py`
- `src/agents/experiment_designer/tools/validate_twin_fidelity_tool.py`

**Tasks:**
- [ ] Wire `simulate_intervention_tool.py` to Experiment Designer agent
  - Import `SimulationEngine` class
  - Implement tool input schema validation
  - Call `run_intervention_simulation()`
  - Return formatted result to agent
- [ ] Wire `validate_twin_fidelity_tool.py` to Experiment Designer agent
  - Import `FidelityTracker` class
  - Implement validation logic
  - Return fidelity report to agent
- [ ] Register tools in agent's tool list
- [ ] Add tool usage examples to agent prompts

**Testing Requirements:**
- Unit tests for tool input/output schemas
- Integration test: agent uses tool in workflow

---

## Section 2: Tool Composer Pipelines

**Status:** Python modules integrated, database ready, LLM integration pending
**Priority:** High
**Depends on:** LLM API key, Supabase connection, tool implementations

### 2.1 Decomposer Phase Integration

**File:** `src/agents/tool_composer/decomposer.py`

**Tasks:**
- [ ] Configure LLM client (OpenAI/Anthropic) in `__init__()`
- [ ] Implement `decompose_query()` with LLM prompting
  - Load decomposition prompt from `prompts.py`
  - Call LLM with structured output format
  - Parse sub-questions from LLM response
- [ ] Implement `identify_dependencies()` with dependency detector
  - Import `DependencyDetector` from classifier
  - Analyze sub-question relationships
  - Return dependency graph (REFERENCE_CHAIN, CONDITIONAL, etc.)
- [ ] Add query caching/deduplication logic
  - Check `composer_episodes` for similar past queries
  - Use `find_similar_compositions()` function
- [ ] Persist to database
  - Save sub-questions to `composer_episodes.sub_questions` (JSONB)
  - Save dependencies to `composer_episodes.dependencies` (JSONB)

**Testing Requirements:**
- Decomposition accuracy on 10 sample multi-faceted queries
- Dependency detection precision/recall
- LLM cost monitoring (tokens per query)

---

### 2.2 Planner Phase Integration

**File:** `src/agents/tool_composer/planner.py`

**Tasks:**
- [ ] Implement `create_execution_plan()`
  - Load tools from `tool_registry` table
  - Map sub-questions to appropriate tools by category
  - Build dependency-aware execution DAG
- [ ] Implement `optimize_plan()` using `get_tool_execution_order()`
  - Call Supabase function for topological sort
  - Identify parallelizable tool groups
  - Estimate total latency from `v_tool_reliability` view
- [ ] Add plan validation
  - Check all sub-questions are covered
  - Verify tool input/output schemas are compatible
  - Detect circular dependencies
- [ ] Persist to database
  - Save plan to `composer_episodes.tool_plan` (JSONB)
  - Save parallelizable groups to `composer_episodes.parallelizable_groups`

**Testing Requirements:**
- Plan generation for various query types
- Validation: reject invalid tool combinations
- Performance: plan latency < 500ms

---

### 2.3 Executor Phase Integration

**File:** `src/agents/tool_composer/executor.py`

**Tasks:**
- [ ] Implement `execute_plan()` orchestration
  - Load execution plan from database
  - Initialize `composition_steps` for each tool call
  - Execute tools in topological order
- [ ] Implement parallel execution for independent tools
  - Use asyncio or threading for parallelizable groups
  - Timeout handling (30s per tool)
  - Retry logic (max 2 retries)
- [ ] Implement data flow between tools
  - Extract output from producer tool
  - Transform via `tool_dependencies.transform_expression` (optional)
  - Inject into consumer tool input
- [ ] Add execution monitoring
  - Update `composition_steps.status` (PENDING → EXECUTING → COMPLETED/FAILED)
  - Track latency for each step
  - Log errors with stack traces
- [ ] Persist results
  - Save outputs to `composition_steps.output_result` (JSONB)
  - Aggregate into `composer_episodes.tool_outputs`
- [ ] Trigger performance logging
  - `trg_log_step_performance` trigger auto-logs to `tool_performance`

**Testing Requirements:**
- Execute 5 sample compositions end-to-end
- Verify parallel execution speedup
- Test error recovery (retry, timeout)
- Validate data flow between dependent tools

---

### 2.4 Synthesizer Phase Integration

**File:** `src/agents/tool_composer/synthesizer.py`

**Tasks:**
- [ ] Configure LLM client for synthesis
- [ ] Implement `synthesize_response()`
  - Load tool outputs from `composer_episodes.tool_outputs`
  - Load original sub-questions
  - Construct synthesis prompt combining all results
  - Call LLM to generate coherent final answer
- [ ] Add citation/source tracking
  - Include which tools contributed to each part of answer
  - Add confidence indicators
- [ ] Persist final response
  - Save to `composer_episodes.synthesized_response` (TEXT)
  - Update status to COMPLETED
  - Record total latency across all phases

**Testing Requirements:**
- Synthesis quality evaluation (human review)
- Verify all sub-questions addressed
- LLM cost monitoring

---

### 2.5 Composer Main Orchestrator

**File:** `src/agents/tool_composer/composer.py`

**Tasks:**
- [ ] Implement `compose()` main entry point
  - Initialize episode in database
  - Call decomposer → planner → executor → synthesizer
  - Handle phase transitions
  - Update `composer_episodes.status` at each phase
- [ ] Add query embedding generation
  - Use OpenAI embeddings API
  - Store in `composer_episodes.query_embedding` (vector 1536)
- [ ] Add session tracking
  - Link to `session_id` for conversation context
  - Link to `classification_id` from orchestrator
- [ ] Implement timeout handling
  - Overall composition timeout (5 minutes)
  - Mark as TIMEOUT status if exceeded
- [ ] Add feedback collection hooks
  - Placeholder for user rating (1-5)
  - Feedback text storage

**Testing Requirements:**
- End-to-end composition tests
- Timeout behavior validation
- Session continuity tests

---

## Section 3: Orchestrator Classifier

**Status:** Python modules integrated, database ready, LLM integration pending
**Priority:** High
**Depends on:** LLM API key, Supabase connection, domain vocabulary

### 3.1 Feature Extractor Integration

**File:** `src/agents/orchestrator/classifier/feature_extractor.py`

**Tasks:**
- [ ] Implement `extract_features()` rule-based extraction
  - Question word detection (what, why, how, which)
  - Multi-part query detection (conjunctions: "and", "but", "then")
  - Named entity recognition (HCP, territory, segment)
  - Temporal references (comparison periods)
  - Causal keywords ("effect", "impact", "cause")
- [ ] Add feature normalization
  - Return structured feature dict
- [ ] Persist features
  - Save to `classification_logs.features_extracted` (JSONB)

**Testing Requirements:**
- Feature extraction accuracy on 20 sample queries
- Edge case handling (ambiguous queries)

---

### 3.2 Domain Mapper Integration

**File:** `src/agents/orchestrator/classifier/domain_mapper.py`

**Tasks:**
- [ ] Load domain vocabulary from `config/domain_vocabulary_v4.2.0.yaml`
- [ ] Implement `map_to_domains()` keyword matching
  - Score each domain (CAUSAL, SEGMENTATION, GAP, etc.)
  - Use TF-IDF or keyword overlap
  - Return top 1-3 domains with confidence scores
- [ ] Add LLM fallback for ambiguous cases
  - Prompt LLM with query + domain descriptions
  - Parse domain classification from LLM
- [ ] Persist domain mapping
  - Save to `classification_logs.domain_mapping` (JSONB)

**Testing Requirements:**
- Domain classification accuracy > 85%
- Validate multi-domain queries (e.g., causal + gap)

---

### 3.3 Dependency Detector Integration

**File:** `src/agents/orchestrator/classifier/dependency_detector.py`

**Tasks:**
- [ ] Implement `detect_dependencies()` for multi-part queries
  - Pronoun reference detection ("those", "that", "them")
  - Conditional logic detection ("if...then")
  - Logical sequence detection (cause → effect → intervention)
  - Entity transformation detection (filtered/subset references)
- [ ] Return dependency graph with types
  - Array of {from: "Q1", to: "Q2", type: "REFERENCE_CHAIN"}
- [ ] Persist dependencies
  - Save to `classification_logs.dependency_analysis` (JSONB)

**Testing Requirements:**
- Dependency detection on 15 multi-part queries
- Validate all 4 dependency types recognized

---

### 3.4 Pattern Selector Integration

**File:** `src/agents/orchestrator/classifier/pattern_selector.py`

**Tasks:**
- [ ] Implement `select_routing_pattern()` decision logic
  - **SINGLE_AGENT:** 1 domain, no dependencies → route to primary agent
  - **PARALLEL_DELEGATION:** Multiple domains, no dependencies → route to multiple agents in parallel
  - **TOOL_COMPOSER:** Multiple domains with dependencies → use Tool Composer
  - **CLARIFICATION_NEEDED:** Ambiguous/insufficient info → request user clarification
- [ ] Calculate confidence score (0-1)
- [ ] Add LLM confirmation for edge cases
- [ ] Return routing decision + target agents
- [ ] Persist classification
  - Save to `classification_logs.routing_pattern`
  - Save to `classification_logs.target_agents` (TEXT[])
  - Save to `classification_logs.confidence`

**Testing Requirements:**
- Pattern selection accuracy on 30 diverse queries
- Validate all 4 routing patterns triggered
- Confidence calibration (high confidence = correct)

---

### 3.5 Classification Pipeline Integration

**File:** `src/agents/orchestrator/classifier/pipeline.py`

**Tasks:**
- [ ] Implement `classify_query()` orchestrator
  - Stage 1: Feature extraction
  - Stage 2: Domain mapping
  - Stage 3: Dependency detection
  - Stage 4: Pattern selection
  - Record latency for each stage
- [ ] Add query hash generation (SHA-256)
- [ ] Check for duplicate queries
  - Query `classification_logs` by hash
  - Return cached classification if recent (< 24h)
- [ ] Persist full classification log
  - Create row in `classification_logs` table
  - Return `classification_id` for linking
- [ ] Add feedback loop
  - Placeholder for `was_correct` feedback
  - Learning from corrections

**Testing Requirements:**
- End-to-end classification pipeline
- Performance: classification latency < 1 second
- Cache hit rate monitoring

---

## Section 4: API Endpoints

**Status:** Not started
**Priority:** Medium
**Depends on:** All above components functional, FastAPI/Flask setup

### 4.1 Digital Twin API Endpoints

**Suggested Routes:**

```
POST   /api/v1/digital-twins/generate
POST   /api/v1/digital-twins/{twin_id}/simulate
GET    /api/v1/digital-twins/{twin_id}/fidelity
GET    /api/v1/digital-twins/
DELETE /api/v1/digital-twins/{twin_id}
```

**Tasks:**
- [ ] Create FastAPI router in `src/api/routes/digital_twins.py`
- [ ] Implement POST `/generate` endpoint
  - Accept twin type (hcp/patient/territory) + criteria
  - Call `TwinGenerator.generate_twin()`
  - Return twin_id and match quality metrics
- [ ] Implement POST `/{twin_id}/simulate` endpoint
  - Accept intervention parameters
  - Call `SimulationEngine.run_intervention_simulation()`
  - Return predicted outcome + recommendation
- [ ] Implement GET `/{twin_id}/fidelity` endpoint
  - Call `FidelityTracker.get_latest_fidelity()`
  - Return fidelity grade + metrics
- [ ] Implement GET `/` list endpoint with pagination
- [ ] Implement DELETE `/{twin_id}` soft delete
- [ ] Add authentication/authorization middleware
- [ ] Add request validation (Pydantic models)
- [ ] Add error handling (404, 400, 500)

**Testing Requirements:**
- API integration tests (pytest + httpx)
- Load testing (100 concurrent requests)
- OpenAPI spec generation (Swagger docs)

---

### 4.2 Tool Composer API Endpoints

**Suggested Routes:**

```
POST   /api/v1/compose
GET    /api/v1/compositions/{composition_id}
GET    /api/v1/compositions/{composition_id}/steps
POST   /api/v1/compositions/{composition_id}/feedback
GET    /api/v1/compositions/
```

**Tasks:**
- [ ] Create FastAPI router in `src/api/routes/compositions.py`
- [ ] Implement POST `/compose` endpoint
  - Accept user query text
  - Optionally accept session_id for context
  - Call `Composer.compose()` asynchronously
  - Return composition_id immediately
  - Stream updates via WebSocket (optional)
- [ ] Implement GET `/{composition_id}` status endpoint
  - Return current status, progress, partial results
  - Support polling for completion
- [ ] Implement GET `/{composition_id}/steps` endpoint
  - Return detailed execution trace
  - Show tool calls, latencies, outputs
- [ ] Implement POST `/{composition_id}/feedback` endpoint
  - Accept user rating (1-5) + feedback text
  - Update `composer_episodes.success`, `user_rating`, `feedback_text`
- [ ] Implement GET `/` list endpoint with filters
  - Filter by status, session_id, date range
  - Pagination
- [ ] Add WebSocket support for real-time updates (optional)

**Testing Requirements:**
- API integration tests
- Async execution testing
- WebSocket connection testing (if implemented)

---

### 4.3 Orchestrator Classifier API Endpoints

**Suggested Routes:**

```
POST   /api/v1/classify
GET    /api/v1/classifications/{classification_id}
POST   /api/v1/classifications/{classification_id}/feedback
GET    /api/v1/classifications/accuracy
```

**Tasks:**
- [ ] Create FastAPI router in `src/api/routes/classifications.py`
- [ ] Implement POST `/classify` endpoint
  - Accept user query text
  - Call `ClassificationPipeline.classify_query()`
  - Return routing_pattern, target_agents, confidence
- [ ] Implement GET `/{classification_id}` endpoint
  - Return full classification log (features, domains, dependencies)
- [ ] Implement POST `/{classification_id}/feedback` endpoint
  - Accept was_correct (bool) + correct_pattern (if wrong)
  - Update `classification_logs` for learning
- [ ] Implement GET `/accuracy` analytics endpoint
  - Query `v_classification_accuracy` view
  - Return metrics by pattern over time
- [ ] Add caching middleware for repeated queries

**Testing Requirements:**
- API integration tests
- Feedback loop validation
- Accuracy tracking over time

---

### 4.4 Tool Registry API Endpoints (Admin)

**Suggested Routes:**

```
GET    /api/v1/admin/tools
POST   /api/v1/admin/tools
PUT    /api/v1/admin/tools/{tool_id}
DELETE /api/v1/admin/tools/{tool_id}
GET    /api/v1/admin/tools/reliability
POST   /api/v1/admin/tools/update-metrics
```

**Tasks:**
- [ ] Create admin router in `src/api/routes/admin.py`
- [ ] Implement CRUD endpoints for tool_registry
- [ ] Implement GET `/reliability` endpoint
  - Query `v_tool_reliability` view
  - Return performance metrics
- [ ] Implement POST `/update-metrics` endpoint
  - Call `update_tool_registry_metrics()` function
  - Return updated metrics
- [ ] Add admin authentication (JWT + role check)

**Testing Requirements:**
- Admin role enforcement tests
- Metrics update validation

---

## Section 5: Testing

**Status:** Not started
**Priority:** High
**Depends on:** Components implemented above

### 5.1 Unit Tests

**Directory:** `tests/unit/`

**Tasks:**
- [ ] Digital Twin tests
  - `test_twin_generator.py`: Propensity model, matching logic
  - `test_simulation_engine.py`: Simulation calculations, recommendations
  - `test_fidelity_tracker.py`: Fidelity metrics, grading logic
- [ ] Tool Composer tests
  - `test_decomposer.py`: Query decomposition, dependency detection
  - `test_planner.py`: Plan generation, optimization
  - `test_executor.py`: Tool execution, data flow
  - `test_synthesizer.py`: Response synthesis
- [ ] Orchestrator Classifier tests
  - `test_feature_extractor.py`: Feature extraction accuracy
  - `test_domain_mapper.py`: Domain classification accuracy
  - `test_dependency_detector.py`: Dependency graph creation
  - `test_pattern_selector.py`: Routing pattern selection
- [ ] Tool Registry tests
  - `test_tool_registry.py`: Tool discovery, schema validation
  - `test_tool_dependencies.py`: Dependency resolution

**Testing Requirements:**
- Code coverage > 80%
- All tests pass in CI/CD pipeline
- Mock external dependencies (LLM, database)

---

### 5.2 Integration Tests

**Directory:** `tests/integration/`

**Tasks:**
- [ ] End-to-end composition test
  - Submit multi-faceted query → verify correct final response
  - Validate database state (episodes, steps, performance logs)
- [ ] End-to-end digital twin test
  - Generate twin → run simulation → validate fidelity
  - Verify MLflow logging
- [ ] API integration tests
  - Test all endpoints with real database
  - Validate request/response schemas
- [ ] Database integration tests
  - Test triggers fire correctly
  - Test views return expected data
  - Test functions execute without errors

**Testing Requirements:**
- Use test database (not production)
- Automated setup/teardown
- Tests run in < 5 minutes

---

### 5.3 Performance Tests

**Directory:** `tests/performance/`

**Tasks:**
- [ ] Composition latency benchmarks
  - Measure P50, P95, P99 latencies for various query types
  - Target: P95 < 10 seconds for complex compositions
- [ ] Tool execution benchmarks
  - Measure individual tool latencies
  - Update `tool_registry.avg_latency_ms` baselines
- [ ] API load tests
  - Simulate 100 concurrent users
  - Monitor error rates, latency
- [ ] Database query optimization
  - Profile slow queries (> 100ms)
  - Add indexes if needed

**Testing Requirements:**
- Benchmark reports (markdown or HTML)
- Automated regression detection (latency increases > 20%)

---

### 5.4 Test Data Setup

**Directory:** `tests/fixtures/`

**Tasks:**
- [ ] Create sample HCP/patient dataset
  - 1000 synthetic HCP records
  - 5000 synthetic patient records
  - Realistic distributions for demographics, prescriptions
- [ ] Create sample tool execution results
  - Mock outputs for each of 13 tools
  - Compatible with tool input/output schemas
- [ ] Create sample queries for classification
  - 50 queries covering all routing patterns
  - Labels for accuracy testing
- [ ] Create database seed script
  - Load fixtures into test database
  - Script: `tests/setup_test_data.py`

---

## Section 6: Monitoring & Observability

**Status:** Not started
**Priority:** Medium
**Depends on:** Components deployed in production

### 6.1 Logging

**Tasks:**
- [ ] Configure structured logging (JSON format)
  - Use Python `logging` module + `structlog`
  - Log levels: DEBUG, INFO, WARNING, ERROR
- [ ] Add correlation IDs
  - Link logs across composition phases
  - Link logs to `session_id`, `composition_id`, `classification_id`
- [ ] Configure log aggregation
  - Send logs to centralized service (e.g., CloudWatch, Datadog)
- [ ] Add log sampling for high-volume endpoints

---

### 6.2 Metrics

**Tasks:**
- [ ] Instrument key metrics with Prometheus/StatsD
  - Composition latency (histogram)
  - Classification accuracy (gauge)
  - Tool execution counts (counter)
  - Active compositions (gauge)
- [ ] Create Grafana dashboards
  - Composition success rate over time
  - Tool reliability metrics (`v_tool_reliability` view)
  - Classification accuracy (`v_classification_accuracy` view)
  - Active compositions (`v_active_compositions` view)
- [ ] Set up alerting
  - Alert if composition success rate < 80%
  - Alert if tool failure rate > 10%
  - Alert if classification accuracy drops > 5%

---

### 6.3 Tracing

**Tasks:**
- [ ] Add distributed tracing with OpenTelemetry
  - Trace composition phases (decompose → plan → execute → synthesize)
  - Trace tool executions
  - Trace LLM calls
- [ ] Configure trace backend (Jaeger, Datadog APM)
- [ ] Add trace sampling (10% of requests)

---

## Section 7: Documentation

**Status:** Partially complete
**Priority:** Medium

### 7.1 User Documentation

**Tasks:**
- [ ] Create user guide for Tool Composer
  - How to submit multi-faceted queries
  - How to interpret composition results
  - Examples of effective queries
- [ ] Create user guide for Digital Twin
  - How to generate twins
  - How to run simulations
  - How to interpret fidelity metrics
- [ ] Create API reference documentation
  - OpenAPI spec (auto-generated)
  - Code examples for each endpoint

---

### 7.2 Developer Documentation

**Tasks:**
- [ ] Architecture diagrams
  - Tool Composer 4-phase pipeline
  - Orchestrator 4-stage classifier
  - Digital Twin workflow
- [ ] Database schema documentation
  - ERD diagrams for new tables
  - Table/column descriptions (already in SQL comments)
- [ ] Deployment guide
  - Environment variables
  - Database setup
  - MLflow configuration
- [ ] Contributing guide
  - How to add new tools to registry
  - How to extend classifier domains

---

## Section 8: Deployment

**Status:** Not started
**Priority:** Low (until implementation complete)

### 8.1 Environment Configuration

**Tasks:**
- [ ] Create `.env.example` template
  - Supabase connection string
  - OpenAI/Anthropic API key
  - MLflow tracking URI
  - Environment (dev/staging/prod)
- [ ] Document required environment variables in README
- [ ] Set up secrets management (AWS Secrets Manager, Vault)

---

### 8.2 Database Migrations

**Tasks:**
- [ ] ✅ Migration 012 applied to Supabase (digital twin tables)
- [ ] ✅ Migration 013 applied to Supabase (tool composer tables)
- [ ] Create migration rollback scripts (if needed)
- [ ] Document migration process in deployment guide

---

### 8.3 CI/CD Pipeline

**Tasks:**
- [ ] Set up GitHub Actions workflows
  - Workflow: Run tests on PR
  - Workflow: Deploy to staging on merge to `develop`
  - Workflow: Deploy to production on merge to `main`
- [ ] Add linting/formatting checks (black, ruff, mypy)
- [ ] Add security scanning (bandit, safety)
- [ ] Add dependency vulnerability scanning (Dependabot)

---

### 8.4 Production Deployment

**Tasks:**
- [ ] Deploy API server (FastAPI)
  - Containerize with Docker
  - Deploy to AWS ECS/Kubernetes/Cloud Run
- [ ] Deploy background workers (for async compositions)
  - Use Celery + Redis or AWS SQS
- [ ] Set up database connection pooling
  - Use PgBouncer or Supabase pooler
- [ ] Configure auto-scaling
  - Scale based on CPU/memory usage
  - Scale based on queue depth (for workers)

---

## Priority Recommendations

**Week 1-2: Core Functionality**
1. Digital Twin ML algorithms (Section 1)
2. Tool Composer pipelines (Section 2)
3. Orchestrator classifier (Section 3)

**Week 3-4: API & Testing**
4. API endpoints (Section 4)
5. Unit tests (Section 5.1)
6. Integration tests (Section 5.2)

**Week 5-6: Production Readiness**
7. Monitoring & observability (Section 6)
8. Performance testing (Section 5.3)
9. Documentation (Section 7)
10. Deployment (Section 8)

---

## Notes

- **LLM Costs:** Monitor OpenAI/Anthropic usage. Composition queries can be expensive (decomposition + synthesis = ~2000 tokens). Consider caching similar queries.
- **Database Performance:** The `composer_episodes` table will grow quickly. Consider partitioning by month after 100k rows.
- **Vector Search:** The `ivfflat` index on embeddings is approximate. For production, consider upgrading to `hnsw` index (requires pgvector >= 0.5.0).
- **Tool Registry:** The 13 seeded tools are placeholders. You'll need to implement the actual Python functions for each tool and register them in the `ToolRegistry` class.
- **MLflow:** The Digital Twin uses MLflow for experiment tracking. Ensure MLflow server is accessible and properly configured.

---

## Success Criteria

The v4.2.0 implementation is complete when:

- [ ] A user can submit a multi-faceted query and receive a synthesized response
- [ ] The classifier correctly routes queries to appropriate patterns (>85% accuracy)
- [ ] Digital twins can be generated and simulations run end-to-end
- [ ] All API endpoints are functional with <5% error rate
- [ ] Test coverage >80%, all tests passing
- [ ] Production deployment is stable with <1% downtime
- [ ] Documentation is complete and published

---

**Last Updated:** 2024-12-17
**Maintained By:** E2I Development Team
**Questions?** See `docs/INTEGRATION_SUMMARY_V4.2.md` for architecture overview.
