# =============================================================================
# E2I Causal Analytics - Secure Production Docker Compose
# =============================================================================
# Network-isolated configuration with proper security segmentation.
# Must stay in sync with docker-compose.yml + docker-compose.opik.yml.
#
# Network Architecture:
#   ┌─────────────────────────────────────────────────────────────────────────┐
#   │                           EXTERNAL                                       │
#   │                              │                                           │
#   │                       ┌──────▼──────┐                                   │
#   │                       │   nginx     │ ← public-network (exposed)         │
#   │                       └──────┬──────┘                                   │
#   │                              │                                           │
#   │              ┌───────────────┼───────────────┐                          │
#   │              │               │               │                          │
#   │       ┌──────▼──────┐ ┌──────▼──────┐ ┌──────▼──────┐                   │
#   │       │  frontend   │ │     api     │ │   workers   │ ← app-network     │
#   │       └─────────────┘ └──────┬──────┘ └──────┬──────┘                   │
#   │                              │               │                          │
#   │              ┌───────────────┴───────────────┘                          │
#   │              │                                                          │
#   │    ┌─────────┴─────────┐                ┌─────────────────┐            │
#   │    │                   │                │                  │            │
#   │  ┌─▼─────────┐ ┌───────▼─────┐    ┌────▼────┐ ┌──────────▼──┐         │
#   │  │  redis    │ │  falkordb   │    │  mlflow │ │  bentoml   │         │
#   │  └───────────┘ └─────────────┘    └─────────┘ └─────────────┘         │
#   │        └─── data-network (internal) ──┘  └─ monitoring-network ─┘      │
#   └─────────────────────────────────────────────────────────────────────────┘
#
# Usage:
#   docker compose -f docker/docker-compose.secure.yml up -d
# =============================================================================

name: e2i-secure

# =============================================================================
# NETWORKS - Security-Segmented Architecture
# =============================================================================
networks:
  public-network:
    driver: bridge
    name: e2i-public
    driver_opts:
      com.docker.network.bridge.enable_icc: "false"
    labels:
      - "e2i.network.type=public"
      - "e2i.network.exposure=external"

  app-network:
    driver: bridge
    name: e2i-app
    labels:
      - "e2i.network.type=application"
      - "e2i.network.exposure=internal"

  data-network:
    driver: bridge
    name: e2i-data
    internal: true
    labels:
      - "e2i.network.type=data"
      - "e2i.network.exposure=internal-only"

  monitoring-network:
    driver: bridge
    name: e2i-monitoring
    labels:
      - "e2i.network.type=monitoring"
      - "e2i.network.exposure=internal"

  celery-network:
    driver: bridge
    name: e2i-celery
    internal: true
    labels:
      - "e2i.network.type=celery"
      - "e2i.network.exposure=internal-only"

# =============================================================================
# VOLUMES
# =============================================================================
volumes:
  redis_data:
    driver: local
    name: e2i_redis_data
  falkordb_data:
    driver: local
    name: e2i_falkordb_data
  mlflow_db:
    driver: local
    name: e2i_mlflow_db
  mlflow_artifacts:
    driver: local
    name: e2i_mlflow_artifacts
  bentoml_models:
    driver: local
    name: e2i_bentoml_models
  nginx_logs:
    driver: local
    name: e2i_nginx_logs

  # Shared volumes
  ml_artifacts:
    driver: local
    name: e2i_ml_artifacts
  causal_outputs:
    driver: local
    name: e2i_causal_outputs
  agent_outputs:
    driver: local
    name: e2i_agent_outputs
  model_registry:
    driver: local
    name: e2i_model_registry
  feature_cache:
    driver: local
    name: e2i_feature_cache
  experiment_designs:
    driver: local
    name: e2i_experiment_designs

  # Observability volumes
  prometheus_data:
    driver: local
    name: e2i_prometheus_data
  loki_data:
    driver: local
    name: e2i_loki_data
  grafana_data:
    driver: local
    name: e2i_grafana_data

# =============================================================================
# SERVICES
# =============================================================================
services:
  # ===========================================================================
  # CONFIGURATION VALIDATION
  # ===========================================================================
  config-check:
    image: alpine:3.19
    container_name: e2i_config_check
    entrypoint: /bin/sh
    command:
      - -c
      - |
        FAIL=0
        for VAR_NAME in REDIS_PASSWORD FALKORDB_PASSWORD; do
          eval VAL="\$$VAR_NAME"
          if [ "$$VAL" = "changeme" ] || [ -z "$$VAL" ]; then
            if [ "$$ENVIRONMENT" = "production" ]; then
              echo "ERROR: $$VAR_NAME must be set to a secure value in production"
              FAIL=1
            else
              echo "WARNING: $$VAR_NAME is unset or default - acceptable for development only"
            fi
          fi
        done
        if [ "$$FAIL" = "1" ]; then exit 1; fi
        echo "Configuration validation passed"
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD:?REDIS_PASSWORD must be set}
      FALKORDB_PASSWORD: ${FALKORDB_PASSWORD:?FALKORDB_PASSWORD must be set}
      ENVIRONMENT: ${ENVIRONMENT:-production}
    restart: "no"
    networks: []

  # ===========================================================================
  # TIER 0: REVERSE PROXY (Public Network Only)
  # ===========================================================================
  nginx:
    image: nginx:alpine
    container_name: e2i-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.secure.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
    networks:
      - public-network
      - app-network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /var/cache/nginx:size=10M
      - /var/run:size=5M
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
    depends_on:
      api:
        condition: service_healthy
      frontend:
        condition: service_healthy
    labels:
      - "e2i.tier=0"
      - "e2i.component=proxy"
      - "e2i.network.access=public,app"

  # ===========================================================================
  # TIER 1: DATA STORES (Data Network - Internal Only)
  # ===========================================================================
  redis:
    image: redis:7-alpine
    container_name: e2i-redis
    restart: unless-stopped
    volumes:
      - redis_data:/data
    command: >
      redis-server
      --appendonly yes
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --requirepass ${REDIS_PASSWORD:?REDIS_PASSWORD must be set}
      --protected-mode yes
      --bind 0.0.0.0
    networks:
      - data-network
      - celery-network
    security_opt:
      - no-new-privileges:true
    depends_on:
      config-check:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    labels:
      - "e2i.tier=1"
      - "e2i.component=cache"
      - "e2i.network.access=data,celery"

  falkordb:
    image: falkordb/falkordb:v4.14.11
    container_name: e2i-falkordb
    restart: unless-stopped
    volumes:
      - falkordb_data:/data
    environment:
      - FALKORDB_ARGS=--maxmemory 1gb --protected-mode yes --requirepass ${FALKORDB_PASSWORD:?FALKORDB_PASSWORD must be set}
    networks:
      - data-network
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "6379", "-a", "${FALKORDB_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    labels:
      - "e2i.tier=1"
      - "e2i.component=graph"
      - "e2i.network.access=data"

  # ===========================================================================
  # TIER 2: MLOPS SERVICES (Monitoring Network)
  # ===========================================================================
  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.1.0
    container_name: e2i-mlflow
    restart: unless-stopped
    expose:
      - "5000"
    volumes:
      - mlflow_db:/mlflow/db
      - mlflow_artifacts:/mlflow/artifacts
      - ml_artifacts:/mlflow/ml_artifacts:ro
      - model_registry:/mlflow/model_registry
    environment:
      - MLFLOW_TRACKING_URI=sqlite:///mlflow/db/mlflow.db
      - MLFLOW_ARTIFACT_ROOT=/mlflow/artifacts
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow/db/mlflow.db
      --default-artifact-root /mlflow/artifacts
      --host 0.0.0.0
      --port 5000
    networks:
      - monitoring-network
      - app-network
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5000/')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    labels:
      - "e2i.tier=2"
      - "e2i.component=mlflow"
      - "e2i.network.access=monitoring,app"

  bentoml:
    build:
      context: ..
      dockerfile: docker/bentoml/Dockerfile
      target: production
    container_name: e2i-bentoml
    restart: unless-stopped
    expose:
      - "3000"
    volumes:
      - bentoml_models:/home/bentoml/bentos
      - model_registry:/models:ro
    environment:
      - BENTOML_HOME=/home/bentoml
      - BENTOML_PORT=3000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - monitoring-network
      - app-network
    security_opt:
      - no-new-privileges:true
    depends_on:
      mlflow:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    labels:
      - "e2i.tier=2"
      - "e2i.component=bentoml"
      - "e2i.network.access=monitoring,app"

  feast:
    build:
      context: .
      dockerfile: Dockerfile.feast
    container_name: e2i-feast
    restart: unless-stopped
    expose:
      - "6566"
    volumes:
      - ../feature_repo:/feast
      - feature_cache:/feast/data
    environment:
      - FEAST_OFFLINE_STORE_TYPE=file
      - FEAST_ONLINE_STORE_TYPE=redis
      - FEAST_REDIS_HOST=redis
      - FEAST_REDIS_PORT=6379
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6566/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - monitoring-network
      - data-network
    security_opt:
      - no-new-privileges:true
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    labels:
      - "e2i.tier=2"
      - "e2i.component=feast"
      - "e2i.network.access=monitoring,data"

  # ===========================================================================
  # TIER 3: APPLICATION SERVICES (App Network)
  # ===========================================================================
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: production
    container_name: e2i-api
    restart: unless-stopped
    expose:
      - "8000"
    volumes:
      - ml_artifacts:/app/data/ml_artifacts
      - causal_outputs:/app/data/causal_outputs
      - agent_outputs:/app/data/agent_outputs
      - experiment_designs:/app/data/experiment_designs
      - feature_cache:/app/data/feature_cache:ro
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - FALKORDB_URL=redis://:${FALKORDB_PASSWORD}@falkordb:6379/0
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - BENTOML_URL=http://bentoml:3000
      - FEAST_URL=http://feast:6566
      - OPIK_URL=http://opik-backend:8080
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/1
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD}@redis:6379/2
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
      - WORKERS=4
      - CORS_ORIGINS=${CORS_ORIGINS}
      - ENABLE_HSTS=true
    command: >
      gunicorn src.api.main:app
      --workers 4
      --worker-class uvicorn.workers.UvicornWorker
      --bind 0.0.0.0:8000
      --timeout 120
      --graceful-timeout 30
      --keep-alive 5
      --max-requests 1000
      --max-requests-jitter 50
      --access-logfile -
      --error-logfile -
    networks:
      - app-network
      - data-network
      - monitoring-network
      - celery-network
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 5G
        reservations:
          cpus: '1'
          memory: 2G
    depends_on:
      redis:
        condition: service_healthy
      falkordb:
        condition: service_healthy
      mlflow:
        condition: service_healthy
    labels:
      - "e2i.tier=3"
      - "e2i.component=api"
      - "e2i.network.access=app,data,monitoring,celery"

  frontend:
    build:
      context: ..
      dockerfile: docker/frontend/Dockerfile
      target: production
    container_name: e2i-frontend
    restart: unless-stopped
    expose:
      - "80"
    networks:
      - app-network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /var/cache/nginx:size=10M
      - /var/run:size=5M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    depends_on:
      api:
        condition: service_healthy
    labels:
      - "e2i.tier=3"
      - "e2i.component=frontend"
      - "e2i.network.access=app"

  # ===========================================================================
  # TIER 4: CELERY WORKERS (Celery + Data + Monitoring Networks)
  # ===========================================================================
  worker-light:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: production
    restart: unless-stopped
    volumes:
      - ml_artifacts:/app/data/ml_artifacts
      - causal_outputs:/app/data/causal_outputs
      - agent_outputs:/app/data/agent_outputs
      - experiment_designs:/app/data/experiment_designs
      - feature_cache:/app/data/feature_cache:ro
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - FALKORDB_URL=redis://:${FALKORDB_PASSWORD}@falkordb:6379/0
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - BENTOML_URL=http://bentoml:3000
      - FEAST_URL=http://feast:6566
      - OPIK_URL=http://opik-backend:8080
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/1
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD}@redis:6379/2
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
      - WORKER_TYPE=light
    command: >
      celery -A src.workers.celery_app worker
      --loglevel=INFO
      --concurrency=4
      --queues=default,quick,api
      --prefetch-multiplier=2
      --max-tasks-per-child=100
      --hostname=worker_light@%h
    networks:
      - celery-network
      - data-network
      - monitoring-network
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD", "celery", "-A", "src.workers.celery_app", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
      replicas: 2
    depends_on:
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    labels:
      - "e2i.tier=4"
      - "e2i.component=worker"
      - "e2i.worker.tier=light"
      - "e2i.network.access=celery,data,monitoring"

  worker-medium:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: production
    restart: unless-stopped
    volumes:
      - ml_artifacts:/app/data/ml_artifacts
      - causal_outputs:/app/data/causal_outputs
      - agent_outputs:/app/data/agent_outputs
      - experiment_designs:/app/data/experiment_designs
      - feature_cache:/app/data/feature_cache:ro
    tmpfs:
      - /app/tmp:size=2G,mode=1770
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - FALKORDB_URL=redis://:${FALKORDB_PASSWORD}@falkordb:6379/0
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - BENTOML_URL=http://bentoml:3000
      - FEAST_URL=http://feast:6566
      - OPIK_URL=http://opik-backend:8080
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/1
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD}@redis:6379/2
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
      - WORKER_TYPE=medium
    command: >
      celery -A src.workers.celery_app worker
      --loglevel=INFO
      --concurrency=2
      --queues=analytics,reports,aggregations
      --prefetch-multiplier=1
      --max-tasks-per-child=50
      --hostname=worker_medium@%h
    networks:
      - celery-network
      - data-network
      - monitoring-network
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD", "celery", "-A", "src.workers.celery_app", "inspect", "ping"]
      interval: 45s
      timeout: 15s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
      replicas: 1
    depends_on:
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    labels:
      - "e2i.tier=4"
      - "e2i.component=worker"
      - "e2i.worker.tier=medium"
      - "e2i.network.access=celery,data,monitoring"

  worker-heavy:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: production
    restart: unless-stopped
    volumes:
      - ml_artifacts:/app/data/ml_artifacts
      - causal_outputs:/app/data/causal_outputs
      - agent_outputs:/app/data/agent_outputs
      - experiment_designs:/app/data/experiment_designs
      - feature_cache:/app/data/feature_cache:ro
    tmpfs:
      - /app/tmp:size=8G,mode=1770
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - FALKORDB_URL=redis://:${FALKORDB_PASSWORD}@falkordb:6379/0
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - BENTOML_URL=http://bentoml:3000
      - FEAST_URL=http://feast:6566
      - OPIK_URL=http://opik-backend:8080
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/1
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD}@redis:6379/2
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
      - WORKER_TYPE=heavy
      - OMP_NUM_THREADS=8
      - OPENBLAS_NUM_THREADS=8
    command: >
      celery -A src.workers.celery_app worker
      --loglevel=INFO
      --concurrency=2
      --queues=shap,causal,ml,twins
      --prefetch-multiplier=1
      --max-tasks-per-child=10
      --time-limit=3600
      --soft-time-limit=3300
      --hostname=worker_heavy@%h
    networks:
      - celery-network
      - data-network
      - monitoring-network
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD", "celery", "-A", "src.workers.celery_app", "inspect", "ping"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G
        reservations:
          cpus: '4'
          memory: 8G
      replicas: 0
    depends_on:
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    labels:
      - "e2i.tier=4"
      - "e2i.component=worker"
      - "e2i.worker.tier=heavy"
      - "e2i.network.access=celery,data,monitoring"
      - "e2i.autoscale=true"

  scheduler:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: production
    container_name: e2i-scheduler
    restart: unless-stopped
    volumes:
      - causal_outputs:/app/data/causal_outputs:ro
    environment:
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/1
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD}@redis:6379/2
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
    command: >
      celery -A src.workers.celery_app beat
      --loglevel=INFO
      --scheduler=celery.beat:PersistentScheduler
    healthcheck:
      test: ["CMD-SHELL", "celery -A src.workers.celery_app inspect ping --timeout 10 2>/dev/null | grep -q pong || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 30s
    networks:
      - celery-network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    depends_on:
      redis:
        condition: service_healthy
      worker-light:
        condition: service_healthy
    labels:
      - "e2i.tier=4"
      - "e2i.component=scheduler"
      - "e2i.network.access=celery"

  # ===========================================================================
  # TIER 5: OBSERVABILITY STACK
  # ===========================================================================
  prometheus:
    image: prom/prometheus:v3.2.1
    container_name: e2i-prometheus
    restart: unless-stopped
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    networks:
      - monitoring-network
      - app-network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp:size=10M,mode=1770
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    labels:
      - "e2i.tier=5"
      - "e2i.component=prometheus"

  alertmanager:
    image: prom/alertmanager:v0.28.1
    container_name: e2i-alertmanager
    restart: unless-stopped
    volumes:
      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    networks:
      - monitoring-network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "e2i.tier=5"
      - "e2i.component=alertmanager"

  node-exporter:
    image: prom/node-exporter:v1.9.0
    container_name: e2i-node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - monitoring-network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "e2i.tier=5"
      - "e2i.component=node-exporter"

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.16.0
    container_name: e2i-postgres-exporter
    restart: unless-stopped
    environment:
      DATA_SOURCE_NAME: ${SUPABASE_DB_URL:?SUPABASE_DB_URL must be set for postgres-exporter}
    networks:
      - monitoring-network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "e2i.tier=5"
      - "e2i.component=postgres-exporter"

  loki:
    image: grafana/loki:3.4.2
    container_name: e2i-loki
    restart: unless-stopped
    volumes:
      - ./loki/loki-config.yml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - monitoring-network
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    labels:
      - "e2i.tier=5"
      - "e2i.component=loki"

  promtail:
    image: grafana/promtail:3.4.2
    container_name: e2i-promtail
    restart: unless-stopped
    volumes:
      - ./promtail/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - monitoring-network
    security_opt:
      - no-new-privileges:true
    depends_on:
      loki:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "e2i.tier=5"
      - "e2i.component=promtail"

  grafana:
    image: grafana/grafana:11.5.2
    container_name: e2i-grafana
    restart: unless-stopped
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:?GRAFANA_ADMIN_PASSWORD must be set}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3200
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - monitoring-network
      - app-network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp:size=10M,mode=1770
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    labels:
      - "e2i.tier=5"
      - "e2i.component=grafana"

  # ===========================================================================
  # TIER 6: DEBUG TOOLS (Optional - profiles)
  # ===========================================================================
  falkordb-browser:
    image: falkordb/falkordb-browser:v1.7.1
    container_name: e2i-falkordb-browser
    restart: unless-stopped
    ports:
      - "127.0.0.1:3030:3000"
    environment:
      - FALKORDB_HOST=falkordb
      - FALKORDB_PORT=6379
    networks:
      - data-network
    profiles:
      - debug
    security_opt:
      - no-new-privileges:true
    depends_on:
      falkordb:
        condition: service_healthy
    labels:
      - "e2i.tier=6"
      - "e2i.component=debug"
      - "e2i.network.access=data"

  redis-commander:
    image: rediscommander/redis-commander:0.8.1
    container_name: e2i-redis-commander
    restart: unless-stopped
    ports:
      - "127.0.0.1:8081:8081"
    environment:
      - REDIS_HOSTS=e2i:redis:6379:0:${REDIS_PASSWORD}
    networks:
      - data-network
    profiles:
      - debug
    security_opt:
      - no-new-privileges:true
    depends_on:
      redis:
        condition: service_healthy
    labels:
      - "e2i.tier=6"
      - "e2i.component=debug"
      - "e2i.network.access=data"
