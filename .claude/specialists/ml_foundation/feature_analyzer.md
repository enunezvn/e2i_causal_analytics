# CLAUDE.md - Feature Analyzer Agent (Hybrid)

## Overview

The **Feature Analyzer** is a **Hybrid agent** that combines SHAP computation with LLM-based interpretation. It analyzes trained models to explain feature importance, detect interactions, and generate natural language explanations for stakeholders.

| Attribute | Value |
|-----------|-------|
| **Tier** | 0 (ML Foundation) |
| **Type** | **Hybrid** (Computation + LLM) |
| **SLA** | <120 seconds |
| **Primary Output** | SHAPAnalysis, FeatureImpacts, InterpretabilityReport |
| **Database Table** | `ml_shap_analyses` |
| **Memory Types** | Working, Semantic |
| **MLOps Tools** | SHAP |

## Hybrid Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│  FEATURE ANALYZER (HYBRID AGENT)                                │
│                                                                 │
│  ┌─────────────┐     ┌─────────────┐     ┌─────────────┐       │
│  │   Node 1    │     │   Node 2    │     │   Node 3    │       │
│  │   SHAP      │ ──▶ │ Interaction │ ──▶ │    LLM      │       │
│  │ Computation │     │  Detection  │     │Interpretation│      │
│  │  (DoWhy)    │     │  (NumPy)    │     │  (Claude)   │       │
│  └─────────────┘     └─────────────┘     └─────────────┘       │
│                                                                 │
│  NO LLM ─────────────────────────────────────────▶ LLM         │
└─────────────────────────────────────────────────────────────────┘
```

## Responsibilities

1. **Global SHAP**: Compute feature importance across entire dataset
2. **Local SHAP**: Explain individual predictions
3. **Interaction Detection**: Identify feature interaction effects
4. **Segment Analysis**: SHAP differences across segments (region, brand)
5. **NL Interpretation**: Generate human-readable explanations
6. **Semantic Memory**: Store feature relationships for causal_impact

## Position in Pipeline

```
┌──────────────────┐
│  model_trainer   │
│  (Trained Model) │
└────────┬─────────┘
         │ TrainedModel
         ▼
┌──────────────────┐
│ feature_analyzer │ ◀── YOU ARE HERE (HYBRID)
│  (SHAP + NL)     │
└────────┬─────────┘
         │ SHAPAnalysis
         ├──────────────────────▶ explainer (Tier 5)
         ├──────────────────────▶ causal_impact (Tier 2)
         ▼
┌──────────────────┐
│  model_deployer  │
└──────────────────┘
```

## Outputs

### SHAPAnalysis

```python
@dataclass
class SHAPAnalysis:
    """SHAP value analysis results."""
    analysis_id: str
    training_run_id: str
    experiment_id: str
    
    # Global Importance
    global_importance: Dict[str, float]
    # {"call_frequency": 0.23, "recency_days": 0.18, ...}
    
    global_importance_ranked: List[Tuple[str, float]]
    # [("call_frequency", 0.23), ("recency_days", 0.18), ...]
    
    # Directional Effects
    feature_directions: Dict[str, str]
    # {"call_frequency": "positive", "competitor_share": "negative"}
    
    # Interaction Matrix
    interaction_matrix: Dict[str, Dict[str, float]]
    # {"call_frequency": {"recency_days": 0.05, ...}, ...}
    
    top_interactions: List[Tuple[str, str, float]]
    # [("call_frequency", "recency_days", 0.05), ...]
    
    # Segment Analysis
    segment_shap: Dict[str, Dict[str, float]]
    # {"northeast": {"call_frequency": 0.25}, "south": {...}}
    
    # Raw SHAP Values (for visualization)
    shap_values_sample: np.ndarray  # Sample for plotting
    feature_names: List[str]
    
    # Metadata
    samples_analyzed: int
    computation_time_seconds: float
    computed_at: datetime
```

### InterpretabilityReport

```python
@dataclass
class InterpretabilityReport:
    """Natural language interpretation of SHAP analysis."""
    analysis_id: str
    
    # Executive Summary
    executive_summary: str
    # "The model primarily relies on engagement metrics, with call frequency
    #  being the strongest predictor of prescription likelihood..."
    
    # Feature Explanations
    feature_explanations: Dict[str, str]
    # {"call_frequency": "Higher call frequency increases prescription
    #   probability. Each additional monthly call increases likelihood by ~3%"}
    
    # Key Insights
    key_insights: List[str]
    # ["Engagement metrics dominate (60% of model signal)",
    #  "Strong interaction between recency and frequency"]
    
    # Actionable Recommendations
    recommendations: List[str]
    # ["Prioritize HCPs with low recent engagement but high historical calls",
    #  "Regional targeting should focus on Northeast outliers"]
    
    # Cautions
    cautions: List[str]
    # ["Correlation between competitor_share and outcome may be confounded"]
    
    # Generated By
    generated_by: str  # "claude-sonnet-4-20250514"
```

## Database Schema

### ml_shap_analyses Table

```sql
CREATE TABLE ml_shap_analyses (
    analysis_id TEXT PRIMARY KEY,
    training_run_id TEXT REFERENCES ml_training_runs(training_run_id),
    experiment_id TEXT REFERENCES ml_experiments(experiment_id),
    
    -- Global Analysis
    global_importance JSONB NOT NULL,
    feature_directions JSONB,
    
    -- Interactions
    interaction_matrix JSONB,
    top_interactions JSONB,
    
    -- Segment Analysis
    segment_shap JSONB,
    
    -- Interpretation (LLM-generated)
    executive_summary TEXT,
    feature_explanations JSONB,
    key_insights JSONB,
    recommendations JSONB,
    
    -- Metadata
    samples_analyzed INTEGER,
    computation_time_seconds NUMERIC(8,2),
    analyzed_by agent_name_enum DEFAULT 'feature_analyzer',
    analyzed_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_shap_experiment ON ml_shap_analyses(experiment_id);
```

## Implementation

### agent.py

```python
from src.agents.base_agent import BaseAgent
from src.mlops.shap_explainer import SHAPExplainer
from src.memory.semantic_memory import SemanticMemory
from .shap_computer import SHAPComputer
from .interaction_detector import InteractionDetector
from .importance_narrator import ImportanceNarrator

class FeatureAnalyzerAgent(BaseAgent):
    """
    Feature Analyzer: SHAP computation + LLM interpretation.
    
    HYBRID AGENT: Combines computation (Nodes 1-2) with LLM (Node 3).
    """
    
    tier = 0
    tier_name = "ml_foundation"
    agent_type = "hybrid"  # HYBRID: Computation + LLM
    sla_seconds = 120
    
    def __init__(self, config: AgentConfig):
        super().__init__(config)
        self.shap_computer = SHAPComputer()
        self.interaction_detector = InteractionDetector()
        self.narrator = ImportanceNarrator()
        self.semantic_memory = SemanticMemory(agent="feature_analyzer")
        self.shap_repo = MLSHAPAnalysisRepository()
    
    async def execute(self, state: AgentState) -> AgentState:
        """
        Hybrid execution: Computation → Interpretation.
        
        Node 1: SHAP Computation (no LLM)
        Node 2: Interaction Detection (no LLM)
        Node 3: NL Interpretation (LLM)
        """
        trained_model = state.trained_model
        experiment_id = state.experiment_id
        
        # Load model and sample data
        model = await self._load_model(trained_model.model_artifact_uri)
        X_sample = await self._load_sample_data(experiment_id, n=1000)
        
        # ═══════════════════════════════════════════════════════════
        # NODE 1: SHAP Computation (NO LLM)
        # ═══════════════════════════════════════════════════════════
        start_time = time.time()
        
        shap_values, expected_value = await self.shap_computer.compute_global(
            model=model,
            X=X_sample,
            algorithm=trained_model.algorithm_name
        )
        
        global_importance = self.shap_computer.compute_importance(
            shap_values=shap_values,
            feature_names=X_sample.columns.tolist()
        )
        
        feature_directions = self.shap_computer.compute_directions(
            shap_values=shap_values,
            X=X_sample
        )
        
        # ═══════════════════════════════════════════════════════════
        # NODE 2: Interaction Detection (NO LLM)
        # ═══════════════════════════════════════════════════════════
        interaction_matrix = await self.interaction_detector.compute(
            shap_values=shap_values,
            X=X_sample
        )
        
        top_interactions = self.interaction_detector.get_top_interactions(
            interaction_matrix=interaction_matrix,
            top_k=10
        )
        
        # Segment analysis
        segment_shap = await self._compute_segment_shap(
            model=model,
            experiment_id=experiment_id
        )
        
        computation_time = time.time() - start_time
        
        # Build SHAP analysis object
        shap_analysis = SHAPAnalysis(
            analysis_id=f"shap_{trained_model.training_run_id}",
            training_run_id=trained_model.training_run_id,
            experiment_id=experiment_id,
            global_importance=global_importance,
            global_importance_ranked=sorted(
                global_importance.items(), key=lambda x: x[1], reverse=True
            ),
            feature_directions=feature_directions,
            interaction_matrix=interaction_matrix,
            top_interactions=top_interactions,
            segment_shap=segment_shap,
            shap_values_sample=shap_values[:100],  # Sample for viz
            feature_names=X_sample.columns.tolist(),
            samples_analyzed=len(X_sample),
            computation_time_seconds=computation_time,
            computed_at=datetime.utcnow()
        )
        
        # ═══════════════════════════════════════════════════════════
        # NODE 3: NL Interpretation (LLM)
        # ═══════════════════════════════════════════════════════════
        interpretability_report = await self.narrator.generate_interpretation(
            shap_analysis=shap_analysis,
            scope_spec=state.scope_spec,
            validation_metrics=state.validation_metrics
        )
        
        # ═══════════════════════════════════════════════════════════
        # Store in Semantic Memory (for causal_impact)
        # ═══════════════════════════════════════════════════════════
        await self._update_semantic_memory(
            shap_analysis=shap_analysis,
            experiment_id=experiment_id
        )
        
        # Persist to database
        await self.shap_repo.create(shap_analysis, interpretability_report)
        
        return state.with_updates(
            shap_analysis=shap_analysis,
            interpretability_report=interpretability_report
        )
    
    async def _update_semantic_memory(
        self,
        shap_analysis: SHAPAnalysis,
        experiment_id: str
    ):
        """Store feature relationships in semantic memory."""
        
        # Store top feature relationships
        for feature, importance in shap_analysis.global_importance_ranked[:10]:
            await self.semantic_memory.add_relationship(
                source="prediction_target",
                target=feature,
                relationship="influenced_by",
                weight=importance,
                context={"experiment_id": experiment_id}
            )
        
        # Store feature interactions
        for feat1, feat2, strength in shap_analysis.top_interactions[:5]:
            await self.semantic_memory.add_relationship(
                source=feat1,
                target=feat2,
                relationship="interacts_with",
                weight=strength,
                context={"experiment_id": experiment_id}
            )
```

### shap_computer.py

```python
import shap

class SHAPComputer:
    """Compute SHAP values for model interpretation."""
    
    async def compute_global(
        self,
        model,
        X: pd.DataFrame,
        algorithm: str
    ) -> Tuple[np.ndarray, float]:
        """Compute global SHAP values."""
        
        # Select appropriate explainer
        if algorithm in ["XGBoost", "LightGBM", "CatBoost"]:
            explainer = shap.TreeExplainer(model)
        elif algorithm in ["CausalForest", "LinearDML"]:
            # For EconML models, use KernelExplainer
            explainer = shap.KernelExplainer(
                model.predict, 
                shap.sample(X, 100)
            )
        else:
            explainer = shap.Explainer(model, X)
        
        shap_values = explainer.shap_values(X)
        
        # Handle multi-output (binary classification)
        if isinstance(shap_values, list):
            shap_values = shap_values[1]  # Positive class
        
        expected_value = explainer.expected_value
        if isinstance(expected_value, list):
            expected_value = expected_value[1]
        
        return shap_values, expected_value
    
    def compute_importance(
        self,
        shap_values: np.ndarray,
        feature_names: List[str]
    ) -> Dict[str, float]:
        """Compute global feature importance from SHAP values."""
        importance = np.abs(shap_values).mean(axis=0)
        
        # Normalize to sum to 1
        importance = importance / importance.sum()
        
        return dict(zip(feature_names, importance.tolist()))
    
    def compute_directions(
        self,
        shap_values: np.ndarray,
        X: pd.DataFrame
    ) -> Dict[str, str]:
        """Determine if features have positive or negative effect."""
        directions = {}
        
        for i, col in enumerate(X.columns):
            # Correlation between feature value and SHAP value
            corr = np.corrcoef(X[col].values, shap_values[:, i])[0, 1]
            directions[col] = "positive" if corr > 0 else "negative"
        
        return directions
    
    async def compute_local(
        self,
        model,
        X: pd.DataFrame,
        instance_idx: int
    ) -> Dict[str, float]:
        """Compute SHAP values for a single instance."""
        explainer = shap.Explainer(model, X)
        shap_values = explainer.shap_values(X.iloc[[instance_idx]])
        
        return dict(zip(X.columns, shap_values[0].tolist()))
```

### interaction_detector.py

```python
class InteractionDetector:
    """Detect feature interactions from SHAP values."""
    
    async def compute(
        self,
        shap_values: np.ndarray,
        X: pd.DataFrame
    ) -> Dict[str, Dict[str, float]]:
        """Compute interaction matrix."""
        n_features = shap_values.shape[1]
        feature_names = X.columns.tolist()
        
        interaction_matrix = {}
        
        for i in range(n_features):
            interaction_matrix[feature_names[i]] = {}
            for j in range(n_features):
                if i != j:
                    # Compute interaction strength via correlation of SHAP values
                    interaction = np.abs(np.corrcoef(
                        shap_values[:, i], 
                        shap_values[:, j]
                    )[0, 1])
                    
                    interaction_matrix[feature_names[i]][feature_names[j]] = interaction
        
        return interaction_matrix
    
    def get_top_interactions(
        self,
        interaction_matrix: Dict,
        top_k: int = 10
    ) -> List[Tuple[str, str, float]]:
        """Get top K feature interactions."""
        interactions = []
        
        for feat1, inner in interaction_matrix.items():
            for feat2, strength in inner.items():
                if feat1 < feat2:  # Avoid duplicates
                    interactions.append((feat1, feat2, strength))
        
        return sorted(interactions, key=lambda x: x[2], reverse=True)[:top_k]
```

### importance_narrator.py (LLM Node)

```python
class ImportanceNarrator:
    """Generate NL interpretations using LLM."""
    
    async def generate_interpretation(
        self,
        shap_analysis: SHAPAnalysis,
        scope_spec: ScopeSpec,
        validation_metrics: ValidationMetrics
    ) -> InterpretabilityReport:
        """Generate human-readable interpretation."""
        
        # Build prompt with SHAP results
        prompt = self._build_interpretation_prompt(
            shap_analysis=shap_analysis,
            scope_spec=scope_spec,
            validation_metrics=validation_metrics
        )
        
        # Call LLM
        response = await self.llm_client.generate(
            system_prompt=INTERPRETATION_SYSTEM_PROMPT,
            user_prompt=prompt,
            response_format=InterpretabilityReportSchema
        )
        
        return InterpretabilityReport(
            analysis_id=shap_analysis.analysis_id,
            executive_summary=response.executive_summary,
            feature_explanations=response.feature_explanations,
            key_insights=response.key_insights,
            recommendations=response.recommendations,
            cautions=response.cautions,
            generated_by="claude-sonnet-4-20250514"
        )

# Prompts
INTERPRETATION_SYSTEM_PROMPT = """You are an ML interpretability expert for pharmaceutical commercial analytics.

Your task is to translate SHAP analysis results into actionable insights for business stakeholders.

CONTEXT:
- Platform: E2I Causal Analytics
- Audience: Pharmaceutical commercial teams (non-technical)
- Goal: Explain what drives model predictions

OUTPUT REQUIREMENTS:
1. Executive summary (2-3 sentences, no jargon)
2. Feature explanations (plain English for each important feature)
3. Key insights (3-5 bullet points)
4. Actionable recommendations (what should the team DO)
5. Cautions (limitations, confounding concerns)

TONE: Professional, clear, actionable. Avoid ML jargon.
"""
```

## Downstream Integration

### explainer (Tier 5)

```python
# In explainer.execute():

# Load SHAP analysis for narrative generation
shap_analysis = await self.shap_repo.get(training_run_id)

# Use feature explanations in user-facing narratives
narrative = self._build_narrative(
    prediction=prediction,
    shap_local=local_shap_values,
    feature_explanations=shap_analysis.feature_explanations
)
```

### causal_impact (Tier 2)

```python
# In causal_impact.execute():

# Query semantic memory for feature relationships
feature_graph = await semantic_memory.query(
    source="prediction_target",
    relationship="influenced_by"
)

# Use in causal graph construction
for edge in feature_graph:
    dag.add_edge(edge.target, "outcome", weight=edge.weight)
```

## Error Handling

```python
class FeatureAnalyzerError(AgentError):
    """Base error for feature_analyzer."""
    pass

class SHAPComputationError(FeatureAnalyzerError):
    """SHAP value computation failed."""
    pass

class InteractionDetectionError(FeatureAnalyzerError):
    """Interaction detection failed."""
    pass

class InterpretationError(FeatureAnalyzerError):
    """LLM interpretation failed."""
    pass
```

## Testing

```python
class TestFeatureAnalyzer:
    
    async def test_shap_computation(self):
        """Test SHAP values computed correctly."""
        result = await agent.execute(state)
        
        assert result.shap_analysis.global_importance is not None
        assert sum(result.shap_analysis.global_importance.values()) == pytest.approx(1.0)
    
    async def test_hybrid_execution(self):
        """Test both computation and LLM nodes execute."""
        result = await agent.execute(state)
        
        # Computation nodes
        assert result.shap_analysis.computation_time_seconds > 0
        
        # LLM node
        assert result.interpretability_report.executive_summary is not None
        assert result.interpretability_report.generated_by == "claude-sonnet-4-20250514"
    
    async def test_semantic_memory_update(self):
        """Test feature relationships stored in semantic memory."""
        await agent.execute(state)
        
        # Query semantic memory
        relationships = await semantic_memory.query(
            source="prediction_target",
            relationship="influenced_by"
        )
        
        assert len(relationships) > 0
```

## Key Principles

1. **Hybrid Separation**: Computation nodes (1-2) have NO LLM calls
2. **LLM for Interpretation**: Node 3 uses LLM only for NL generation
3. **Semantic Memory**: Feature relationships stored for downstream agents
4. **Explainability Focus**: All outputs support stakeholder understanding
5. **Computation Fallback**: If TreeExplainer fails, fall back to KernelExplainer
