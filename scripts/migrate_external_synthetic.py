#!/usr/bin/env python3
"""
Migrate External Synthetic Data System.

Migrates data generated by the deprecated external system (E2i synthetic data/)
to be compatible with the canonical main system (src/ml/synthetic/).

Key Fixes:
1. Brand ENUM case: lowercase -> Capitalized (e.g., 'remibrutinib' -> 'Remibrutinib')
2. Missing holdout split: Re-splits data to include holdout (60/20/15/5)
3. Schema validation: Ensures Supabase compatibility

Usage:
    python scripts/migrate_external_synthetic.py [input_file] [--output output.csv] [--dry-run]
    python scripts/migrate_external_synthetic.py --fix-csv patient_journeys.csv
    python scripts/migrate_external_synthetic.py --validate-only patient_journeys.csv

Options:
    --output      Output file path (default: adds '_migrated' suffix)
    --dry-run     Show what would change without writing files
    --fix-csv     Fix a specific CSV file in place
    --validate-only  Only validate, don't transform
    --verbose     Enable verbose logging
"""

import argparse
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.ml.synthetic.config import BRANDS, DataSplit

# Derive SPLITS from DataSplit enum
SPLITS = [s.value for s in DataSplit]

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


# Brand case mapping (external lowercase -> canonical capitalized)
BRAND_CASE_FIX = {
    "remibrutinib": "Remibrutinib",
    "fabhalta": "Fabhalta",
    "kisqali": "Kisqali",
}

# Split ratios (canonical: 60/20/15/5)
SPLIT_RATIOS = {
    "train": 0.60,
    "validation": 0.20,
    "test": 0.15,
    "holdout": 0.05,
}


def fix_brand_case(df: pd.DataFrame, brand_column: str = "brand") -> Tuple[pd.DataFrame, int]:
    """
    Fix brand ENUM case from lowercase to capitalized.

    Args:
        df: DataFrame with brand column
        brand_column: Name of the brand column

    Returns:
        Tuple of (fixed DataFrame, count of fixes)
    """
    if brand_column not in df.columns:
        logger.warning(f"Column '{brand_column}' not found in DataFrame")
        return df, 0

    df = df.copy()
    original_values = df[brand_column].copy()

    # Apply case fix
    df[brand_column] = df[brand_column].apply(
        lambda x: BRAND_CASE_FIX.get(str(x).lower(), x) if pd.notna(x) else x
    )

    # Count changes
    changes = (original_values != df[brand_column]).sum()

    if changes > 0:
        logger.info(f"Fixed {changes} brand values from lowercase to capitalized")

    return df, changes


def add_holdout_split(
    df: pd.DataFrame,
    split_column: str = "data_split",
    date_column: Optional[str] = None,
    seed: int = 42,
) -> Tuple[pd.DataFrame, Dict[str, int]]:
    """
    Re-split data to include holdout split (60/20/15/5).

    The external system only had 3 splits (train/validation/test).
    This function re-assigns splits to include holdout.

    Args:
        df: DataFrame with split column
        split_column: Name of the split column
        date_column: Optional date column for temporal splitting
        seed: Random seed for reproducibility

    Returns:
        Tuple of (fixed DataFrame, split counts)
    """
    if split_column not in df.columns:
        logger.warning(f"Column '{split_column}' not found in DataFrame")
        return df, {}

    df = df.copy()
    np.random.seed(seed)

    original_splits = df[split_column].value_counts().to_dict()
    logger.info(f"Original splits: {original_splits}")

    # Check if holdout already exists
    if "holdout" in original_splits:
        logger.info("Holdout split already exists, skipping re-split")
        return df, original_splits

    n = len(df)

    if date_column and date_column in df.columns:
        # Temporal split based on date
        df = df.sort_values(date_column)
        indices = df.index.tolist()
    else:
        # Random shuffle for split
        indices = df.index.tolist()
        np.random.shuffle(indices)

    # Calculate split boundaries
    train_end = int(n * SPLIT_RATIOS["train"])
    val_end = train_end + int(n * SPLIT_RATIOS["validation"])
    test_end = val_end + int(n * SPLIT_RATIOS["test"])

    # Assign splits
    new_splits = []
    for i, idx in enumerate(indices):
        if i < train_end:
            new_splits.append("train")
        elif i < val_end:
            new_splits.append("validation")
        elif i < test_end:
            new_splits.append("test")
        else:
            new_splits.append("holdout")

    # Create mapping from index to new split
    split_map = dict(zip(indices, new_splits))
    df[split_column] = df.index.map(split_map)

    new_split_counts = df[split_column].value_counts().to_dict()
    logger.info(f"New splits: {new_split_counts}")

    return df, new_split_counts


def validate_migrated_data(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Validate migrated data against Supabase schema requirements.

    Args:
        df: DataFrame to validate

    Returns:
        Validation results dictionary
    """
    results = {
        "is_valid": True,
        "errors": [],
        "warnings": [],
        "stats": {
            "total_records": len(df),
            "columns": list(df.columns),
        },
    }

    # Validate brand values
    if "brand" in df.columns:
        actual_brands = set(df["brand"].dropna().unique())
        expected_brands = set(BRANDS)

        # Check for invalid brands
        invalid = actual_brands - expected_brands
        if invalid:
            results["errors"].append(f"Invalid brand values: {invalid}")
            results["is_valid"] = False

        # Check for lowercase (migration bug)
        lowercase = [b for b in actual_brands if isinstance(b, str) and b.islower()]
        if lowercase:
            results["errors"].append(f"Lowercase brand values found: {lowercase}")
            results["is_valid"] = False

        results["stats"]["brands"] = {str(k): int(v) for k, v in df["brand"].value_counts().items()}

    # Validate splits
    if "data_split" in df.columns:
        actual_splits = set(df["data_split"].dropna().unique())
        expected_splits = set(SPLITS)

        # Check for missing splits
        missing = expected_splits - actual_splits
        if missing:
            if "holdout" in missing:
                results["errors"].append(f"Missing holdout split (migration required)")
                results["is_valid"] = False
            else:
                results["warnings"].append(f"Missing splits: {missing}")

        results["stats"]["splits"] = {
            str(k): int(v) for k, v in df["data_split"].value_counts().items()
        }

        # Check split ratios
        total = len(df)
        if total > 0:
            ratios = {k: round(v / total * 100, 1) for k, v in results["stats"]["splits"].items()}
            results["stats"]["split_percentages"] = ratios

    return results


def migrate_dataframe(
    df: pd.DataFrame,
    brand_column: str = "brand",
    split_column: str = "data_split",
    date_column: Optional[str] = None,
    seed: int = 42,
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Apply all migrations to a DataFrame.

    Args:
        df: Input DataFrame
        brand_column: Name of brand column
        split_column: Name of split column
        date_column: Optional date column for temporal splitting
        seed: Random seed

    Returns:
        Tuple of (migrated DataFrame, migration report)
    """
    report = {
        "input_records": len(df),
        "brand_fixes": 0,
        "split_changes": False,
        "validation": None,
    }

    logger.info(f"Starting migration of {len(df)} records")

    # Step 1: Fix brand case
    df, brand_fixes = fix_brand_case(df, brand_column)
    report["brand_fixes"] = brand_fixes

    # Step 2: Add holdout split
    df, split_counts = add_holdout_split(df, split_column, date_column, seed)
    report["split_counts"] = split_counts
    report["split_changes"] = "holdout" in split_counts

    # Step 3: Validate
    validation = validate_migrated_data(df)
    report["validation"] = validation

    report["output_records"] = len(df)

    return df, report


def migrate_csv_file(
    input_path: Path,
    output_path: Optional[Path] = None,
    dry_run: bool = False,
) -> Dict[str, Any]:
    """
    Migrate a CSV file from external system format.

    Args:
        input_path: Path to input CSV
        output_path: Path to output CSV (default: adds '_migrated' suffix)
        dry_run: If True, don't write output file

    Returns:
        Migration report
    """
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")

    # Default output path
    if output_path is None:
        output_path = input_path.parent / f"{input_path.stem}_migrated{input_path.suffix}"

    logger.info(f"Reading: {input_path}")
    df = pd.read_csv(input_path)

    # Determine date column if present
    date_column = None
    for col in ["journey_start_date", "created_at", "date", "timestamp"]:
        if col in df.columns:
            date_column = col
            break

    # Migrate
    df_migrated, report = migrate_dataframe(df, date_column=date_column)

    report["input_file"] = str(input_path)
    report["output_file"] = str(output_path)

    if dry_run:
        logger.info("DRY RUN - no files written")
        report["dry_run"] = True
    else:
        logger.info(f"Writing: {output_path}")
        df_migrated.to_csv(output_path, index=False)
        report["dry_run"] = False

    return report


def print_report(report: Dict[str, Any]) -> None:
    """Print a formatted migration report."""
    print("\n" + "=" * 60)
    print("MIGRATION REPORT")
    print("=" * 60)

    print(f"\nInput:  {report.get('input_file', 'DataFrame')}")
    print(f"Output: {report.get('output_file', 'DataFrame')}")
    print(f"Records: {report['input_records']} -> {report['output_records']}")

    print(f"\nBrand Fixes: {report['brand_fixes']}")
    print(f"Split Changes: {report['split_changes']}")

    if report.get("split_counts"):
        print("\nSplit Distribution:")
        for split, count in sorted(report["split_counts"].items()):
            pct = count / report["output_records"] * 100
            print(f"  {split}: {count:,} ({pct:.1f}%)")

    validation = report.get("validation", {})
    if validation:
        status = "VALID" if validation["is_valid"] else "INVALID"
        print(f"\nValidation: {status}")

        if validation.get("errors"):
            print("\nErrors:")
            for error in validation["errors"]:
                print(f"  - {error}")

        if validation.get("warnings"):
            print("\nWarnings:")
            for warning in validation["warnings"]:
                print(f"  - {warning}")

    if report.get("dry_run"):
        print("\n[DRY RUN - no files written]")

    print("=" * 60)


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Migrate external synthetic data to main system format",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Migrate a CSV file
    python scripts/migrate_external_synthetic.py patient_journeys.csv

    # Dry run to see what would change
    python scripts/migrate_external_synthetic.py patient_journeys.csv --dry-run

    # Validate only
    python scripts/migrate_external_synthetic.py patient_journeys.csv --validate-only

    # Fix file in place
    python scripts/migrate_external_synthetic.py --fix-csv patient_journeys.csv
        """,
    )

    parser.add_argument(
        "input_file",
        nargs="?",
        type=Path,
        help="Input CSV file to migrate",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=Path,
        help="Output file path",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show changes without writing files",
    )
    parser.add_argument(
        "--fix-csv",
        type=Path,
        help="Fix a CSV file in place",
    )
    parser.add_argument(
        "--validate-only",
        action="store_true",
        help="Only validate, don't transform",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Enable verbose logging",
    )

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # Handle --fix-csv (in-place fix)
    if args.fix_csv:
        report = migrate_csv_file(args.fix_csv, args.fix_csv, dry_run=args.dry_run)
        print_report(report)
        return 0 if report["validation"]["is_valid"] else 1

    # Require input file for other modes
    if not args.input_file:
        parser.print_help()
        return 1

    # Handle --validate-only
    if args.validate_only:
        logger.info(f"Validating: {args.input_file}")
        df = pd.read_csv(args.input_file)
        validation = validate_migrated_data(df)

        print("\n" + "=" * 60)
        print("VALIDATION REPORT")
        print("=" * 60)
        print(f"File: {args.input_file}")
        print(f"Records: {validation['stats']['total_records']}")
        print(f"Status: {'VALID' if validation['is_valid'] else 'INVALID'}")

        if validation.get("errors"):
            print("\nErrors:")
            for error in validation["errors"]:
                print(f"  - {error}")

        if validation.get("warnings"):
            print("\nWarnings:")
            for warning in validation["warnings"]:
                print(f"  - {warning}")

        if validation["stats"].get("brands"):
            print("\nBrands:")
            for brand, count in validation["stats"]["brands"].items():
                print(f"  {brand}: {count}")

        if validation["stats"].get("splits"):
            print("\nSplits:")
            for split, count in validation["stats"]["splits"].items():
                pct = validation["stats"]["split_percentages"].get(split, 0)
                print(f"  {split}: {count} ({pct}%)")

        print("=" * 60)
        return 0 if validation["is_valid"] else 1

    # Standard migration
    try:
        report = migrate_csv_file(args.input_file, args.output, dry_run=args.dry_run)
        print_report(report)
        return 0 if report["validation"]["is_valid"] else 1
    except Exception as e:
        logger.error(f"Migration failed: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
