# E2I Causal Analytics - Domain Vocabulary Configuration
# ============================================================
# Version: 3.1.0 - Causal Validation Infrastructure
# Last Updated: 2025-12-13
# 
# This file defines the fixed entity vocabularies for the E2I system.
# These vocabularies are used for entity extraction from natural language
# queries WITHOUT requiring medical NER models like BioClinicalBERT.
#
# The values here match the Supabase schema ENUM types exactly.
# ============================================================

# Version tracking
version: "3.1.0"
last_updated: "2025-12-13"
change_notes: |
  V3.1.0 - Causal Validation Infrastructure (2025-12-13):
  - Added causal validation ENUMs for DoWhy refutation tests
  - Added refutation_test_types: placebo_treatment, random_common_cause, 
    data_subset, bootstrap, sensitivity_e_value
  - Added validation_statuses: passed, failed, warning, skipped
  - Added gate_decisions: proceed, review, block
  - Added expert_review_types: dag_approval, methodology_review, 
    quarterly_audit, ad_hoc_validation
  - Added validation-related aliases
  - Added VALIDATION intent indicator for routing
  - Added validation example queries
  - Added ID patterns: validation_id, review_id
  - Table count: 26 → 28 (added causal_validations, expert_reviews)
  
  V3.0.0 - Major Architecture Update (2025-12-08):
  - Added Tier 0: ML Foundation with 7 new agents
  - Total agents: 18 across 6 tiers
  - Added ML-specific intent indicators
  - Added mlops_tools section
  - Added model_stages vocabulary
  - Added data_quality_dimensions
  
  V2.1.0 - Previous:
  - Integrated Dashboard agents + Schema agents into unified 11-agent architecture
  - Added tier classification for agent hierarchy

# ============================================================
# INTEGRATED AGENT ARCHITECTURE (18 Agents in 6 Tiers)
# ============================================================
# 
# Tier 0: ML Foundation (supports all other tiers)
# Tier 1: Coordination
# Tier 2: Causal Analytics (Core E2I Mission)
# Tier 3: Monitoring & Experimentation
# Tier 4: ML & Predictions
# Tier 5: Self-Improvement (Critical for NLV + RAG)
# ============================================================

agents:
  # Tier 0: ML Foundation
  - scope_definer               # Problem scope, success criteria, data requirements
  - data_preparer               # Quality control, baseline metrics, data validation
  - model_selector              # Algorithm evaluation, baseline comparison, architecture
  - model_trainer               # Training pipeline, split enforcement, hyperparameter tuning
  - feature_analyzer            # SHAP values, feature importance, interpretability
  - model_deployer              # Model registry, deployment orchestration, rollback
  - observability_connector     # Opik integration, span emission, quality metrics
  
  # Tier 1: Coordination
  - orchestrator                # Query routing, multi-agent coordination, response synthesis
  
  # Tier 2: Causal Analytics
  - causal_impact               # Chain tracing, effect estimation, path analysis, VALIDATION
  - gap_analyzer                # ROI opportunities, gap prioritization, multiplier discovery
  - heterogeneous_optimizer     # CATE analysis, segment effects, personalization
  
  # Tier 3: Monitoring & Experimentation
  - drift_monitor               # PSI calculation, degradation alerts, causal attribution
  - experiment_designer         # A/B test design, power analysis, causal experiments
  - health_score                # System health, composite metrics, Pareto scoring
  
  # Tier 4: ML & Predictions
  - prediction_synthesizer      # Ensemble coordination, model selection, confidence calibration
  - resource_optimizer          # ROI allocation, budget optimization, effort routing
  
  # Tier 5: Self-Improvement (Critical for NLV + Self-Improving RAG)
  - explainer                   # NL generation, causal narratives, visualization explanations
  - feedback_learner            # DSPy optimization, feedback loops, prompt refinement

# Agent tier classification (for routing and priority)
agent_tiers:
  ml_foundation:
    - scope_definer
    - data_preparer
    - model_selector
    - model_trainer
    - feature_analyzer
    - model_deployer
    - observability_connector
  coordination:
    - orchestrator
  causal_analytics:
    - causal_impact
    - gap_analyzer
    - heterogeneous_optimizer
  monitoring:
    - drift_monitor
    - experiment_designer
    - health_score
  ml_predictions:
    - prediction_synthesizer
    - resource_optimizer
  self_improvement:
    - explainer
    - feedback_learner

# Agent type classification (Standard, Hybrid, Deep)
agent_types:
  standard:
    description: "Tool-heavy, SLA-bound, deterministic computation"
    agents:
      - scope_definer
      - data_preparer
      - model_selector
      - model_trainer
      - model_deployer
      - observability_connector
      - orchestrator
      - gap_analyzer
      - heterogeneous_optimizer
      - drift_monitor
      - health_score
      - prediction_synthesizer
      - resource_optimizer
  hybrid:
    description: "Computation node + LLM interpretation node"
    agents:
      - feature_analyzer      # SHAP computation + NL interpretation
      - causal_impact         # DoWhy/EconML + deep reasoning + VALIDATION
      - experiment_designer   # Power analysis + design strategy reasoning
  deep:
    description: "Extended reasoning, narrative generation"
    agents:
      - explainer
      - feedback_learner

# Agent capabilities mapping (for intent-to-agent routing)
agent_capabilities:
  # Tier 0: ML Foundation
  scope_definer:
    intents: [SCOPE, DEFINE, REQUIREMENTS, CRITERIA, OBJECTIVE]
    description: "Defines prediction problem scope, success criteria, data requirements"
    tier: ml_foundation
    type: standard
    sla_seconds: 5
    outputs: [ScopeSpec, SuccessCriteria, BaselineExpectations]
  data_preparer:
    intents: [QUALITY, VALIDATE, BASELINE, QC, LEAKAGE, READY]
    description: "Quality control before modeling, validates data against scope"
    tier: ml_foundation
    type: standard
    sla_seconds: 60
    outputs: [QCReport, BaselineMetrics, DataReadiness]
  model_selector:
    intents: [ARCHITECTURE, ALGORITHM, SELECT, COMPARE, CANDIDATE]
    description: "Evaluates candidate algorithms, compares to baselines"
    tier: ml_foundation
    type: standard
    sla_seconds: 120
    outputs: [ModelComparison, SelectionRationale, RecommendedModel]
  model_trainer:
    intents: [TRAIN, FIT, TUNE, HYPERPARAMETER, CROSS_VALIDATE]
    description: "Orchestrates training pipeline, enforces ML splits"
    tier: ml_foundation
    type: standard
    sla_seconds: null  # Varies by data size
    outputs: [TrainedModel, ValidationMetrics, TrainingMetadata]
  feature_analyzer:
    intents: [SHAP, IMPORTANCE, INTERPRET, FEATURE, DRIVER, EXPLAIN_MODEL]
    description: "Computes SHAP values, generates feature importance insights"
    tier: ml_foundation
    type: hybrid
    sla_seconds: 120
    outputs: [SHAPAnalysis, FeatureImpacts, InterpretabilityReport]
  model_deployer:
    intents: [DEPLOY, REGISTER, PROMOTE, ROLLBACK, SERVE, ENDPOINT]
    description: "Manages model registry, orchestrates deployments"
    tier: ml_foundation
    type: standard
    sla_seconds: 30
    outputs: [DeploymentManifest, VersionRecord, RollbackPlan]
  observability_connector:
    intents: [OBSERVE, TRACE, SPAN, LATENCY, MONITOR_AGENT, TELEMETRY]
    description: "Emits spans to Opik, tracks quality metrics across agents"
    tier: ml_foundation
    type: standard
    sla_seconds: 0.1  # 100ms async
    outputs: [ObservabilityContext, QualityMetrics, LatencySummary]
  
  # Tier 1: Coordination
  orchestrator:
    intents: [ROUTE, COORDINATE, SYNTHESIZE]
    description: "Coordinates all agents, routes queries, synthesizes responses"
    tier: coordination
    type: standard
    sla_seconds: 2
    outputs: [RoutingDecision, SynthesizedResponse]
  
  # Tier 2: Causal Analytics
  causal_impact:
    # V3.1: Added VALIDATION intent for refutation queries
    intents: [CAUSAL, WHY, EFFECT, CHAIN, COUNTERFACTUAL, WHAT_IF, VALIDATION, REFUTATION]
    description: "Traces causal chains, estimates treatment effects, validates with refutation"
    tier: causal_analytics
    type: hybrid
    sla_seconds: 30
    outputs: [CausalChain, TreatmentEffect, CounterfactualOutcome, ValidationSuite, GateDecision]
  gap_analyzer:
    intents: [GAP, OPPORTUNITY, ROI, PRIORITIZE, MULTIPLIER]
    description: "Identifies performance gaps and ROI opportunities"
    tier: causal_analytics
    type: standard
    sla_seconds: 15
    outputs: [GapReport, ROIPrioritization, MultiplierDiscovery]
  heterogeneous_optimizer:
    intents: [SEGMENT, PERSONALIZE, CATE, SUBGROUP, HETEROGENEOUS]
    description: "Analyzes heterogeneous treatment effects by segment"
    tier: causal_analytics
    type: standard
    sla_seconds: 20
    outputs: [CATEAnalysis, SegmentEffects, TargetingRecommendation]
  
  # Tier 3: Monitoring
  drift_monitor:
    intents: [DRIFT, CHANGE, DEGRADE, ALERT, PSI]
    description: "Monitors for data drift and model degradation"
    tier: monitoring
    type: standard
    sla_seconds: 10
    outputs: [DriftReport, PSIScore, DegradationAlert]
  experiment_designer:
    intents: [EXPERIMENT, TEST, AB_TEST, VALIDATE, POWER, SAMPLE_SIZE]
    description: "Designs A/B tests with causal rigor"
    tier: monitoring
    type: hybrid
    sla_seconds: 30
    outputs: [ExperimentDesign, PowerAnalysis, TestProtocol]
  health_score:
    intents: [HEALTH, STATUS, SCORE, SYSTEM_HEALTH, PARETO]
    description: "Computes system health metrics"
    tier: monitoring
    type: standard
    sla_seconds: 5
    outputs: [HealthScore, ComponentStatus, ParetoAnalysis]
  
  # Tier 4: ML Predictions
  prediction_synthesizer:
    intents: [PREDICT, FORECAST, MODEL, ENSEMBLE, CONFIDENCE]
    description: "Coordinates ML model predictions"
    tier: ml_predictions
    type: standard
    sla_seconds: 10
    outputs: [Prediction, EnsembleResult, ConfidenceInterval]
  resource_optimizer:
    intents: [RESOURCE, ALLOCATE, BUDGET, OPTIMIZE, CAPACITY]
    description: "Optimizes resource allocation by ROI"
    tier: ml_predictions
    type: standard
    sla_seconds: 15
    outputs: [AllocationPlan, BudgetOptimization, CapacityAnalysis]
  
  # Tier 5: Self-Improvement
  explainer:
    intents: [EXPLAIN, DESCRIBE, SUMMARIZE, NARRATIVE, TELL_ME]
    description: "Generates natural language explanations"
    tier: self_improvement
    type: deep
    sla_seconds: 60
    outputs: [Narrative, VisualizationExplanation, CausalStory]
  feedback_learner:
    intents: [LEARN, IMPROVE, FEEDBACK, OPTIMIZE_PROMPT, REFINE]
    description: "Learns from feedback to improve system"
    tier: self_improvement
    type: deep
    sla_seconds: 120
    outputs: [LearningSignal, PromptOptimization, PatternDiscovery]

# ============================================================
# MLOPS TOOLS & INTEGRATIONS
# ============================================================

mlops_tools:
  experiment_tracking:
    name: MLflow
    purpose: "Experiment tracking, model versioning, artifact storage"
    tables: [ml_experiments, ml_model_registry, ml_training_runs]
    agents: [model_trainer, model_selector, model_deployer]
  
  llm_observability:
    name: Opik
    purpose: "LLM/agent observability, span tracing, latency monitoring"
    tables: [ml_observability_spans]
    agents: [observability_connector, all_hybrid_agents, all_deep_agents]
  
  data_quality:
    name: Great Expectations
    purpose: "Data validation, expectation suites, quality reports"
    tables: [ml_data_quality_reports]
    agents: [data_preparer]
  
  feature_store:
    name: Feast
    purpose: "Feature management, point-in-time retrieval, feature serving"
    tables: [ml_feature_store]
    agents: [data_preparer, model_trainer, prediction_synthesizer]
  
  hyperparameter_optimization:
    name: Optuna
    purpose: "Hyperparameter tuning, Bayesian optimization, pruning"
    tables: [ml_training_runs]
    agents: [model_trainer]
  
  explainability:
    name: SHAP
    purpose: "Global/local feature importance, model interpretation"
    tables: [ml_shap_analyses]
    agents: [feature_analyzer, explainer]
  
  model_serving:
    name: BentoML
    purpose: "Model packaging, endpoint serving, batch inference"
    tables: [ml_deployments]
    agents: [model_deployer, prediction_synthesizer]
  
  # V3.1 NEW: Causal validation tools
  causal_validation:
    name: DoWhy
    purpose: "Causal inference, refutation tests, sensitivity analysis"
    tables: [causal_validations, expert_reviews]
    agents: [causal_impact]

# ============================================================
# MODEL STAGES (matches model_stage_enum)
# ============================================================

model_stages:
  - development    # Initial training, experimentation
  - staging        # Validation, pre-production testing
  - shadow         # Running in production without serving traffic
  - production     # Actively serving predictions
  - archived       # Superseded but preserved
  - deprecated     # Scheduled for removal

# ============================================================
# DATA QUALITY DIMENSIONS (matches ml_data_quality_reports)
# ============================================================

data_quality_dimensions:
  completeness:
    description: "Required fields populated above threshold"
    threshold: 0.95
    critical_threshold: 0.985
  validity:
    description: "Values within expected ranges and formats"
    threshold: 0.98
  uniqueness:
    description: "No unexpected duplicates"
    threshold: 0.99
  consistency:
    description: "Cross-field and cross-table consistency"
    threshold: 0.95
  timeliness:
    description: "Data freshness within SLA"
    threshold: 0.90
  accuracy:
    description: "Correctness against source of truth"
    threshold: 0.95

# ============================================================
# V3.1 NEW: CAUSAL VALIDATION VOCABULARIES
# ============================================================

# Refutation test types - matches refutation_test_type ENUM in Supabase
# Used by RefutationRunner in causal_engine/refutation.py
refutation_test_types:
  - placebo_treatment       # Replace treatment with random noise → effect should disappear
  - random_common_cause     # Add fake confounder → effect should remain stable
  - data_subset             # Test on random subsamples → effect should be consistent
  - bootstrap               # Bootstrap resampling → estimate variance
  - sensitivity_e_value     # E-value sensitivity analysis → confounding robustness

# Validation statuses - matches validation_status ENUM in Supabase
# Result of individual refutation tests
validation_statuses:
  - passed                  # Test passed criteria
  - failed                  # Test failed criteria
  - warning                 # Borderline result, review recommended
  - skipped                 # Test not applicable or disabled

# Gate decisions - matches gate_decision ENUM in Supabase
# Aggregate decision from RefutationSuite
gate_decisions:
  - proceed                 # All required tests passed, confidence ≥ 0.7
  - review                  # Partial pass, requires expert review before use
  - block                   # Failed required tests, do not use estimate

# Expert review types - matches expert_review_type ENUM in Supabase
# Types of domain expert validation
expert_review_types:
  - dag_approval            # New DAG structure approval
  - methodology_review      # Causal method validation
  - quarterly_audit         # Scheduled periodic review
  - ad_hoc_validation       # On-demand expert review

# Validation pass criteria (configurable thresholds)
validation_thresholds:
  placebo_treatment:
    max_effect_retention: 0.10     # Effect should drop to <10% of original
    p_value_threshold: 0.05
  random_common_cause:
    max_delta_percent: 15.0        # Effect should change <15%
  data_subset:
    max_variance: 0.20             # Variance across subsets <20%
    min_subsets: 5
  bootstrap:
    ci_coverage: 0.95              # 95% CI should be narrow
    max_cv: 0.30                   # Coefficient of variation <30%
  sensitivity_e_value:
    min_e_value: 1.5               # E-value should be >1.5 for robustness
  
  # Aggregate gate thresholds
  gate_thresholds:
    proceed_min_confidence: 0.70   # Need ≥70% confidence to proceed
    block_max_confidence: 0.40     # Block if confidence <40%
    required_tests: [placebo_treatment, random_common_cause]

# ============================================================
# CORE ENTITY VOCABULARIES
# ============================================================

# Brand types - matches brand_type ENUM in Supabase
brands:
  - Remibrutinib
  - Fabhalta
  - Kisqali
  - competitor
  - other

# Region types - matches region_type ENUM in Supabase
regions:
  - northeast
  - south
  - midwest
  - west

# Specialty types - matches specialty_type in HCP profiles
specialties:
  - allergy
  - hematology
  - oncology
  - cardiology
  - rheumatology
  - immunology

# HCP Segments - Prescriber tiers for filtering
hcp_segments:
  - tier_1       # High prescriber
  - tier_2       # Medium prescriber
  - tier_3       # Low prescriber
  - tier_4       # Non-prescriber

# Journey stages - matches journey_stage_type ENUM in Supabase
journey_stages:
  - diagnosis
  - initial_treatment
  - treatment_optimization
  - maintenance
  - treatment_switch

# Journey status - matches journey_status_type ENUM in Supabase
journey_statuses:
  - active
  - stable
  - transitioning
  - completed

# Priority levels - matches priority_type ENUM in Supabase
priorities:
  - critical
  - high
  - medium
  - low

# Event types - matches event_type ENUM in Supabase
event_types:
  - diagnosis
  - prescription
  - lab_test
  - procedure
  - consultation
  - hospitalization

# Prediction types - matches prediction_type ENUM in Supabase
prediction_types:
  - trigger
  - propensity
  - risk
  - churn
  - next_best_action

# Workstreams - matches workstream_type ENUM in Supabase
workstreams:
  - WS1
  - WS2
  - WS3

# Data splits - matches data_split_type ENUM in Supabase
data_splits:
  - train
  - validation
  - test
  - holdout
  - unassigned

# Split strategies - matches split_strategy_type ENUM in Supabase
split_strategies:
  - chronological
  - patient_stratified
  - rolling_window
  - causal_holdout

# ============================================================
# KPI DEFINITIONS
# ============================================================

kpis:
  # Volume metrics
  - TRx
  - NRx
  - NBRx
  - TRx_share
  
  # Conversion metrics
  - conversion_rate
  - prescription_lift
  - adoption_rate
  
  # Engagement metrics
  - engagement_score
  - digital_engagement
  - call_frequency
  - response_rate
  
  # Performance metrics
  - territory_performance
  - rep_effectiveness
  - goal_attainment
  
  # Causal metrics
  - treatment_effect
  - causal_impact
  - heterogeneous_effect
  - counterfactual_outcome
  
  # V3.1 NEW: Validation metrics
  - refutation_pass_rate
  - validation_confidence
  - e_value

# ============================================================
# ALIASES FOR FUZZY MATCHING
# ============================================================

aliases:
  # Brand aliases
  Remibrutinib:
    - remi
    - remibrutinib
    - rimibrutinib
  Fabhalta:
    - fab
    - fabhalta
  Kisqali:
    - kisq
    - kisqali
    - ribociclib
  
  # Region aliases
  northeast:
    - ne
    - north east
    - new england
    - atlantic
  south:
    - se
    - southeast
    - southern
  midwest:
    - mw
    - mid west
    - central
  west:
    - western
    - pacific
  
  # KPI aliases
  TRx:
    - total rx
    - total prescriptions
    - total scripts
    - prescriptions
  NRx:
    - new rx
    - new prescriptions
    - new scripts
    - new patients
  conversion_rate:
    - conversion
    - cvr
    - convert rate
  engagement_score:
    - engagement
    - eng score
  treatment_effect:
    - ate
    - average treatment effect
    - causal effect
  
  # ─────────────────────────────────────────────────────────────
  # Tier 0: ML Foundation Agent Aliases
  # ─────────────────────────────────────────────────────────────
  scope_definer:
    - scope agent
    - scope definition
    - problem definer
    - requirements agent
    - objective setter
  data_preparer:
    - data prep
    - data preparer
    - qc agent
    - quality agent
    - data validator
    - baseline agent
    - data quality agent
  model_selector:
    - selector agent
    - algorithm selector
    - model chooser
    - architecture agent
    - model comparison
  model_trainer:
    - trainer agent
    - training agent
    - model trainer
    - fit agent
    - tuning agent
    - hyperparameter agent
  feature_analyzer:
    - feature agent
    - shap agent
    - importance agent
    - interpretability agent
    - explainability agent
    - feature importance
  model_deployer:
    - deployer agent
    - deployment agent
    - registry agent
    - model server
    - endpoint agent
    - serving agent
  observability_connector:
    - observability agent
    - opik agent
    - tracing agent
    - telemetry agent
    - span agent
    - monitoring connector
  
  # ─────────────────────────────────────────────────────────────
  # Tier 1-5 Agent Aliases
  # ─────────────────────────────────────────────────────────────
  orchestrator:
    - coordinator
    - router
    - main agent
    - central agent
  causal_impact:
    - causal agent
    - impact agent
    - causal analyzer
    - chain analyzer
    - why agent
    - causal chain analyzer
    - validation agent        # V3.1: Added for refutation queries
  gap_analyzer:
    - gap agent
    - gap analysis
    - opportunity finder
    - roi agent
    - multiplier discoverer
    - multiplier agent
  heterogeneous_optimizer:
    - heterogeneous agent
    - cate agent
    - personalization agent
    - segment optimizer
  drift_monitor:
    - drift agent
    - drift detector
    - degradation monitor
    - data drift monitor
  experiment_designer:
    - experiment agent
    - ab test agent
    - test designer
    - experimentation agent
  health_score:
    - health agent
    - health monitor
    - system health
    - health score agent
  prediction_synthesizer:
    - prediction agent
    - ml agent
    - ensemble agent
    - model coordinator
  resource_optimizer:
    - resource agent
    - allocation agent
    - budget optimizer
    - resource allocation
  explainer:
    - explanation agent
    - narrator
    - explainer agent
    - narrative generator
  feedback_learner:
    - learning agent
    - feedback agent
    - self-improvement agent
    - optimizer agent
  
  # Workstream aliases
  WS1:
    - workstream 1
    - data model
    - ws-1
    - data and model
  WS2:
    - workstream 2
    - triggers
    - hcp triggers
    - ws-2
  WS3:
    - workstream 3
    - implementation
    - impact
    - ws-3
  
  # ─────────────────────────────────────────────────────────────
  # V3.1 NEW: Validation Aliases
  # ─────────────────────────────────────────────────────────────
  placebo_treatment:
    - placebo test
    - placebo refutation
    - null treatment test
    - placebo refuter
  random_common_cause:
    - common cause test
    - confounder test
    - fake confounder
    - random confounder
  data_subset:
    - subset test
    - subsample test
    - data sampling
    - subset refuter
  bootstrap:
    - bootstrap test
    - bootstrap variance
    - resampling test
    - bootstrap refuter
  sensitivity_e_value:
    - e-value
    - e value
    - sensitivity analysis
    - confounding sensitivity
    - sensitivity test
  proceed:
    - pass
    - approved
    - green light
    - valid
    - can proceed
  review:
    - needs review
    - pending review
    - yellow
    - borderline
    - manual review
  block:
    - fail
    - blocked
    - rejected
    - red
    - invalid
    - do not use

# ============================================================
# TIME PERIOD EXPRESSIONS
# ============================================================

time_expressions:
  relative:
    - today
    - yesterday
    - last week
    - last month
    - last quarter
    - last year
    - ytd
    - mtd
    - qtd
    - this week
    - this month
    - this quarter
    - this year

  quarters:
    Q1:
      - q1
      - first quarter
      - jan-mar
    Q2:
      - q2
      - second quarter
      - apr-jun
    Q3:
      - q3
      - third quarter
      - jul-sep
    Q4:
      - q4
      - fourth quarter
      - oct-dec

# ============================================================
# TEMPORAL GRANULARITY CONFIGURATION (Gap 10)
# ============================================================

temporal:
  # Supported granularity levels
  granularities:
    - daily
    - weekly
    - monthly
    - quarterly
    - yearly

  # Fiscal calendar configuration
  fiscal_calendar:
    fiscal_year_start: "02-01"  # February 1st
    quarters:
      Q1: {start: "02-01", end: "04-30", months: [2, 3, 4]}
      Q2: {start: "05-01", end: "07-31", months: [5, 6, 7]}
      Q3: {start: "08-01", end: "10-31", months: [8, 9, 10]}
      Q4: {start: "11-01", end: "01-31", months: [11, 12, 1]}

  # Business calendar settings
  business_calendar:
    timezone: "America/New_York"  # ET for pharmaceutical industry
    exclude_weekends: false        # Include weekend data
    exclude_holidays: false        # Include holiday data
    business_hours:
      start: "09:00"
      end: "17:00"

  # Aggregation rules by granularity
  aggregation_rules:
    daily:
      label_format: "{year}-{month:02d}-{day:02d}"
      group_by: "DATE_TRUNC('day', timestamp_column)"

    weekly:
      start_day: "monday"
      label_format: "Week {week_num}, {year}"
      group_by: "DATE_TRUNC('week', timestamp_column)"

    monthly:
      label_format: "{month_abbr} {year}"
      abbreviations: [Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec]
      group_by: "DATE_TRUNC('month', timestamp_column)"

    quarterly:
      label_format: "Q{quarter} FY{fiscal_year}"
      group_by: "fiscal_quarter(timestamp_column)"

    yearly:
      label_format: "FY{fiscal_year}"
      group_by: "fiscal_year(timestamp_column)"

  # Default lookback windows
  default_lookback:
    daily: 30      # 30 days
    weekly: 12     # 12 weeks (~3 months)
    monthly: 6     # 6 months
    quarterly: 4   # 4 quarters (1 year)
    yearly: 3      # 3 years

  # Time zone handling
  timezone_handling:
    store_as: "UTC"
    display_as: "America/New_York"
    conversion_required: true

# ============================================================
# DASHBOARD FILTER CONFIGURATION (Gap 1)
# ============================================================

# Filter injection rules for NLP queries with active filters
filter_injection:
  # Merge strategy for filter state + NLP extraction
  merge_strategy: "override"
  description: "Filter state always overrides NLP-extracted entities"

  # Null handling rules
  null_handling:
    null_value: "ignore"
    all_value: "ignore"
    description: "Null or 'all' filter values are not injected into entities"

  # Validation rules
  validation:
    validate_against_vocabulary: true
    invalid_value_handling: "reject_and_alert"
    default_on_invalid: null

  # Priority rules
  priority:
    filter_state: 1      # Highest priority
    nlp_extracted: 2     # Lower priority
    default_values: 3    # Lowest priority

# Filter entity type mappings
filter_entity_mappings:
  brand:
    entity_type: "brand"
    vocabulary_key: "brands"
    sql_column: "brand"
    extraction_confidence: 1.0

  region:
    entity_type: "region"
    vocabulary_key: "regions"
    sql_column: "region"
    extraction_confidence: 1.0

  specialty:
    entity_type: "specialty"
    vocabulary_key: "specialties"
    sql_column: "specialty"
    extraction_confidence: 1.0

  hcp_segment:
    entity_type: "hcp_segment"
    vocabulary_key: "hcp_segments"
    sql_column: "prescriber_tier"
    extraction_confidence: 1.0
    value_mapping:
      high_prescriber: "tier_1"
      medium_prescriber: "tier_2"
      low_prescriber: "tier_3"
      non_prescriber: "tier_4"

  date_range:
    entity_type: "temporal_range"
    vocabulary_key: null
    sql_columns:
      start: "event_date >= ?"
      end: "event_date <= ?"
    extraction_confidence: 1.0

# ============================================================
# QUERY INTENT CLASSIFICATION
# ============================================================

intent_indicators:
  # ─────────────────────────────────────────────────────────────
  # Tier 0: ML Foundation Intents
  # ─────────────────────────────────────────────────────────────
  scope_definition:
    keywords:
      - define scope
      - problem definition
      - success criteria
      - requirements
      - objective
      - what should we predict
      - prediction target
    routes_to: scope_definer
  
  data_quality:
    keywords:
      - data quality
      - validate data
      - qc
      - quality check
      - baseline metrics
      - leakage
      - data ready
      - completeness
      - validity
    routes_to: data_preparer
  
  model_selection:
    keywords:
      - which model
      - which algorithm
      - compare models
      - model comparison
      - best algorithm
      - architecture
      - candidate models
      - logistic vs
      - xgboost vs
    routes_to: model_selector
  
  model_training:
    keywords:
      - train model
      - fit model
      - tune
      - hyperparameter
      - cross validation
      - training run
      - optuna
      - mlflow run
    routes_to: model_trainer
  
  feature_importance:
    keywords:
      - shap
      - feature importance
      - which features
      - top features
      - key drivers
      - model interpretation
      - explain model
      - why did model
      - feature contribution
    routes_to: feature_analyzer
  
  deployment:
    keywords:
      - deploy
      - promote
      - rollback
      - model registry
      - endpoint
      - serve model
      - production model
      - staging
      - shadow mode
    routes_to: model_deployer
  
  observability:
    keywords:
      - latency
      - trace
      - span
      - opik
      - agent performance
      - error rate
      - token usage
      - monitoring
      - telemetry
    routes_to: observability_connector
  
  # ─────────────────────────────────────────────────────────────
  # Tier 1-5 Intents
  # ─────────────────────────────────────────────────────────────
  causal:
    keywords:
      - why
      - cause
      - caused
      - because
      - due to
      - result of
      - impact
      - effect
      - influence
      - driver
      - led to
      - contributed
    routes_to: causal_impact
  
  counterfactual:
    keywords:
      - what if
      - what would
      - if we had
      - had we
      - without
      - instead
      - alternatively
      - hypothetically
    routes_to: causal_impact
  
  comparative:
    keywords:
      - compare
      - versus
      - vs
      - difference
      - between
      - better
      - worse
      - higher
      - lower
      - gap
    routes_to: gap_analyzer
  
  predictive:
    keywords:
      - predict
      - forecast
      - will
      - expect
      - project
      - anticipate
      - propensity
      - likelihood
    routes_to: prediction_synthesizer
  
  experimental:
    keywords:
      - test
      - experiment
      - a/b
      - trial
      - pilot
      - sample size
      - power analysis
    routes_to: experiment_designer
  
  optimization:
    keywords:
      - optimize
      - best
      - improve
      - maximize
      - allocate
      - prioritize
    routes_to: resource_optimizer
  
  monitoring:
    keywords:
      - health
      - status
      - drift
      - change
      - trend
      - alert
    routes_to: [health_score, drift_monitor]
  
  segmentation:
    keywords:
      - segment
      - group
      - cohort
      - subgroup
      - personalize
      - heterogeneous
    routes_to: heterogeneous_optimizer
  
  explanation:
    keywords:
      - explain
      - describe
      - summarize
      - tell me about
      - what is
      - how does
    routes_to: explainer
  
  # ─────────────────────────────────────────────────────────────
  # V3.1 NEW: Validation Intent
  # ─────────────────────────────────────────────────────────────
  validation:
    keywords:
      - validate
      - refute
      - refutation
      - robust
      - robustness
      - sensitivity
      - e-value
      - e value
      - placebo
      - pass
      - fail
      - blocked
      - approved
      - gate decision
      - validation status
      - did it pass
      - is it valid
      - can we trust
      - expert review
      - dag approval
    routes_to: causal_impact

# ============================================================
# ID PATTERNS
# ============================================================

id_patterns:
  patient_id: "PAT-\\d{5}"
  hcp_id: "HCP-\\d{5}"
  trigger_id: "TRG-\\d{6}"
  journey_id: "PJ-\\d{8}"
  prediction_id: "PRED-\\d{6}"
  agent_activity_id: "ACT-\\d{8}"
  causal_path_id: "CP-\\d{5}"
  # V4: ML Foundation IDs
  experiment_id: "EXP-\\d{6}"
  model_version_id: "MDL-\\d{6}"
  training_run_id: "RUN-\\d{8}"
  deployment_id: "DEP-\\d{6}"
  # V3.1 NEW: Validation IDs
  validation_id: "VAL-\\d{6}"
  review_id: "REV-\\d{6}"

# ============================================================
# QUERY EXAMPLES (for few-shot prompting)
# ============================================================

example_queries:
  # ─────────────────────────────────────────────────────────────
  # Tier 0: ML Foundation Examples
  # ─────────────────────────────────────────────────────────────
  scope_definition:
    - query: "Define scope for predicting HCP trigger responsiveness"
      intent: SCOPE
      routes_to: scope_definer
      entities:
        prediction_type: trigger
  
  data_quality:
    - query: "Run data quality checks on the training data"
      intent: QUALITY
      routes_to: data_preparer
      entities:
        data_split: train
    
    - query: "Is there any leakage in our feature set?"
      intent: QUALITY
      routes_to: data_preparer
      entities: {}
  
  model_selection:
    - query: "Compare XGBoost vs CausalForest for CATE estimation"
      intent: ARCHITECTURE
      routes_to: model_selector
      entities:
        algorithms: [XGBoost, CausalForest]
    
    - query: "Which algorithm should we use for churn prediction?"
      intent: ARCHITECTURE
      routes_to: model_selector
      entities:
        prediction_type: churn
  
  model_training:
    - query: "Train the propensity model with Optuna tuning"
      intent: TRAINING
      routes_to: model_trainer
      entities:
        prediction_type: propensity
        tool: optuna
    
    - query: "Show me the MLflow runs for experiment EXP-001234"
      intent: TRAINING
      routes_to: model_trainer
      entities:
        experiment_id: EXP-001234
  
  feature_importance:
    - query: "What are the top SHAP features for the Kisqali adoption model?"
      intent: SHAP
      routes_to: feature_analyzer
      entities:
        brands: [Kisqali]
    
    - query: "Explain why this patient has high trigger score"
      intent: INTERPRET
      routes_to: feature_analyzer
      entities:
        prediction_type: trigger
  
  deployment:
    - query: "Promote model MDL-001234 to production"
      intent: DEPLOY
      routes_to: model_deployer
      entities:
        model_version_id: MDL-001234
        target_stage: production
    
    - query: "Rollback the trigger model to the previous version"
      intent: DEPLOY
      routes_to: model_deployer
      entities:
        prediction_type: trigger
        action: rollback
  
  observability:
    - query: "What's the P95 latency for the causal_impact agent?"
      intent: OBSERVE
      routes_to: observability_connector
      entities:
        agent: causal_impact
        metric: latency_p95
    
    - query: "Show me the error rates across all agents today"
      intent: OBSERVE
      routes_to: observability_connector
      entities:
        time_range: today
  
  # ─────────────────────────────────────────────────────────────
  # Tier 1-5 Examples
  # ─────────────────────────────────────────────────────────────
  causal:
    - query: "Why did Remibrutinib performance drop in the Midwest?"
      intent: CAUSAL
      routes_to: causal_impact
      entities:
        brands: [Remibrutinib]
        regions: [midwest]
        kpis: [territory_performance]
    
    - query: "What caused the conversion rate increase in Q3?"
      intent: CAUSAL
      routes_to: causal_impact
      entities:
        kpis: [conversion_rate]
        time_range: {quarter: 3}
  
  exploratory:
    - query: "Show me TRx trends for Kisqali"
      intent: EXPLORATORY
      routes_to: orchestrator
      entities:
        brands: [Kisqali]
        kpis: [TRx]
    
    - query: "What's our current market share by region?"
      intent: EXPLORATORY
      routes_to: orchestrator
      entities:
        kpis: [TRx_share]
  
  comparative:
    - query: "Compare Fabhalta vs Remibrutinib performance in the South"
      intent: COMPARATIVE
      routes_to: gap_analyzer
      entities:
        brands: [Fabhalta, Remibrutinib]
        regions: [south]
        kpis: [territory_performance]
  
  counterfactual:
    - query: "What would TRx look like if we had higher engagement in Northeast?"
      intent: WHAT_IF
      routes_to: causal_impact
      entities:
        kpis: [TRx, engagement_score]
        regions: [northeast]
  
  predictive:
    - query: "Forecast next quarter's NRx for Kisqali"
      intent: PREDICTIVE
      routes_to: prediction_synthesizer
      entities:
        brands: [Kisqali]
        kpis: [NRx]
        time_range: {relative: next_quarter}
  
  experimental:
    - query: "Design an A/B test for the new engagement strategy"
      intent: EXPERIMENTAL
      routes_to: experiment_designer
      entities:
        kpis: [engagement_score]
  
  optimization:
    - query: "How should we reallocate resources across regions for maximum ROI?"
      intent: OPTIMIZATION
      routes_to: resource_optimizer
      entities:
        regions: [northeast, south, midwest, west]
  
  segmentation:
    - query: "Which HCP segments respond best to digital engagement?"
      intent: SEGMENTATION
      routes_to: heterogeneous_optimizer
      entities:
        kpis: [digital_engagement]
  
  health:
    - query: "What's the current system health status?"
      intent: MONITORING
      routes_to: health_score
      entities: {}
  
  explanation:
    - query: "Explain the causal chain between rep visits and prescription lift"
      intent: EXPLANATION
      routes_to: explainer
      entities:
        kpis: [prescription_lift]
  
  # ─────────────────────────────────────────────────────────────
  # V3.1 NEW: Validation Examples
  # ─────────────────────────────────────────────────────────────
  validation:
    - query: "Did the causal estimate pass refutation tests?"
      intent: VALIDATION
      routes_to: causal_impact
      entities:
        validation: [refutation_pass_rate]
    
    - query: "What's the E-value for the engagement effect on TRx?"
      intent: VALIDATION
      routes_to: causal_impact
      entities:
        kpis: [TRx, engagement_score, e_value]
    
    - query: "Which causal estimates are blocked?"
      intent: VALIDATION
      routes_to: causal_impact
      entities:
        gate_decision: [block]
    
    - query: "Show me estimates pending expert review"
      intent: VALIDATION
      routes_to: causal_impact
      entities:
        gate_decision: [review]
    
    - query: "Is the DAG for Kisqali engagement approved?"
      intent: VALIDATION
      routes_to: causal_impact
      entities:
        brands: [Kisqali]
        review_type: dag_approval
    
    - query: "Run refutation suite on estimate CP-00123"
      intent: VALIDATION
      routes_to: causal_impact
      entities:
        causal_path_id: CP-00123
    
    - query: "Can we trust this treatment effect estimate?"
      intent: VALIDATION
      routes_to: causal_impact
      entities: {}

# ============================================================
# MEMORY TYPE MAPPING (for Tri-Memory Architecture)
# ============================================================

memory_types:
  working:
    technology: Redis
    purpose: "Active context, scratchpad, immediate state"
    ttl_seconds: 86400
    agents: all
  
  episodic:
    technology: Supabase/pgvector
    purpose: "What happened - logs of events, training runs, QC reports, validations"
    agents:
      - scope_definer
      - data_preparer
      - model_selector
      - model_trainer
      - model_deployer
      - observability_connector
      - causal_impact          # V3.1: Stores validation history
  
  procedural:
    technology: Supabase/pgvector
    purpose: "How to do things - successful patterns, tool sequences"
    agents:
      - scope_definer
      - data_preparer
      - model_selector
      - model_trainer
      - model_deployer
  
  semantic:
    technology: FalkorDB/Graphity
    purpose: "Known facts and relationships"
    agents:
      - feature_analyzer
      - causal_impact          # Causal graph knowledge + validation status
      - explainer

# ============================================================
# ROUTING PRIORITY (for multi-intent queries)
# ============================================================

routing_priority:
  # Lower number = higher priority
  ml_foundation: 0      # ML lifecycle takes precedence
  coordination: 1       # Orchestrator routing
  causal_analytics: 2   # Core E2I mission (includes validation)
  monitoring: 3         # System health
  ml_predictions: 4     # Predictions
  self_improvement: 5   # Explanations, learning
