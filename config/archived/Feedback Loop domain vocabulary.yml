metadata:
  name: "E2I Domain Vocabulary Extension: Feedback Loop"
  version: "3.2.0"
  extends: "domain_vocabulary v3.1.0"
  purpose: "Define terminology for concept drift detection and ground truth labeling"
  last_updated: "2025-12-23"

overview: |
  This vocabulary extension covers the Feedback Loop Mechanism that enables concept drift detection by assigning ground truth labels to ML predictions after their observation windows elapse. It integrates with the existing Tier 3 drift_monitor agent and feeds labeled data to the Tier 5 feedback_learner agent.

core_concepts:
  label_lag:
    definition: "The inherent delay between when a prediction is made and when the actual outcome (ground truth) becomes observable"
    synonyms: 
      - "outcome delay"
      - "truth lag"
      - "observation latency"
    context: "The churn model has 90-day label lag because we need a full quarter to determine if an HCP actually stopped prescribing"
    related: 
      - "observation_window"
      - "data_source_lag"

  observation_window:
    definition: "The time period after a prediction during which we monitor for the actual outcome"
    synonyms: 
      - "lookforward window"
      - "outcome period"
      - "attribution window"
    unit: "days"
    examples:
      hcp_churn: "90 days"
      script_conversion: "21 days"
      treatment_response: "180 days"
      market_share_impact: "90 days"
      next_best_action: "30 days"
    related: 
      - "label_lag"
      - "min_observation_days"

  ground_truth:
    definition: "The verified actual outcome for a prediction, used to calculate model accuracy and detect concept drift"
    synonyms: 
      - "actual outcome"
      - "true label"
      - "observed outcome"
    storage: "ml_predictions.actual_outcome"
    related: 
      - "truth_source"
      - "truth_confidence"

  concept_drift:
    definition: "A change in the relationship between input features and target outcomes over time, causing model performance degradation even when feature distributions remain stable"
    contrast: "Feature drift (input distribution changes) vs Concept drift (input-output relationship changes)"
    detection: "Requires ground truth labels; cannot be detected with feature drift methods alone (PSI, KS test)"
    metrics: 
      - "accuracy_over_time"
      - "calibration_drift"
      - "class_ratio_shift"
    related: 
      - "feature_drift"
      - "model_degradation"

  feature_drift:
    definition: "A change in the statistical distribution of input features over time"
    synonyms: 
      - "data drift"
      - "covariate shift"
    detection_methods: 
      - "PSI"
      - "KS test"
      - "chi-square"
      - "Jensen-Shannon divergence"
    contrast: "Can be detected without ground truth labels"
    related: 
      - "concept_drift"
      - "psi_threshold"

  truth_confidence:
    definition: "A score (0.0-1.0) indicating how reliable the assigned ground truth label is"
    storage: "ml_predictions.truth_confidence"
    factors:
      - "Data completeness (more prior activity = higher confidence)"
      - "Source reliability (claims data > CRM data)"
      - "Edge case presence (seasonal effects, market disruptions lower confidence)"
    thresholds:
      high: ">=0.90 (include in all analyses)"
      medium: "0.70-0.89 (include with caveats)"
      low: "<0.70 (flag for review, may exclude from drift calculations)"

outcome_labels:
  POSITIVE:
    definition: "The predicted event occurred (e.g., HCP churned, script was written, patient persisted)"
    numeric_value: 1.0
    usage: "Binary classification positive class"
  
  NEGATIVE:
    definition: "The predicted event did not occur"
    numeric_value: 0.0
    usage: "Binary classification negative class"
  
  INDETERMINATE:
    definition: "Insufficient data to reliably determine the outcome"
    numeric_value: null
    causes:
      - "New HCP with <3 prior scripts (no baseline for churn)"
      - "Incomplete data coverage"
      - "Conflicting signals from multiple data sources"
    handling: "Excluded from accuracy calculations, tracked separately"

  EXCLUDED:
    definition: "Prediction deliberately removed from evaluation due to confounding factors"
    numeric_value: null
    causes:
      - "Trigger not delivered (script conversion)"
      - "Market disruption (competitor launch, recall)"
      - "Clinical discontinuation (appropriate therapy stop)"
      - "Therapy switch (patient changed brands, not true discontinuation)"
    handling: "Tracked with exclusion_reason, separate reporting"

  PENDING:
    definition: "Observation window has not yet elapsed; outcome unknown"
    numeric_value: null
    default_state: "All new predictions start as PENDING"
    transition: "Moves to POSITIVE/NEGATIVE/INDETERMINATE/EXCLUDED after feedback loop runs"

truth_definitions:
  hcp_churn_truth:
    question_answered: "Did this HCP actually stop or significantly reduce prescribing?"
    positive_condition: "Zero TRx in observation window with >=1 TRx in prior 90 days, OR TRx decline >70% vs prior period (with >=3 prior scripts)"
    observation_window_days: 90
    data_sources: 
      - "IQVIA APLD (primary)"
      - "Komodo (secondary)"
    edge_cases: 
      - "new_hcp"
      - "seasonal_effects"

  script_conversion_truth:
    question_answered: "Did this HCP write a new script after receiving the trigger?"
    positive_condition: ">=1 NRx within attribution window"
    observation_window_days: 21
    notes: "14-21 day optimal lead time"
    data_sources: 
      - "IQVIA APLD"
      - "HealthVerity"
    edge_cases: 
      - "trigger_not_delivered"
      - "multi_trigger"

  treatment_response_truth:
    question_answered: "Did the patient persist on therapy as predicted?"
    positive_condition: "PDC (Proportion of Days Covered) >=0.80, OR Maximum gap between fills <=60 days"
    observation_window_days: 180
    data_sources: 
      - "HealthVerity (lab/EMR)"
      - "IQVIA APLD (claims)"
    edge_cases: 
      - "therapy_switch"
      - "clinical_discontinuation"

  market_share_impact_truth:
    question_answered: "Did market share change as predicted?"
    outcome_type: "Continuous (actual delta %)"
    measurement: "outcome_share - baseline_share"
    accuracy_threshold: "Within +/- 2 percentage points"
    observation_window_days: 90
    data_sources: 
      - "IQVIA LAAD"
    edge_cases: 
      - "market_disruption"
      - "regional_variance"

  next_best_action_truth:
    question_answered: "Was the recommended action effective?"
    positive_condition: "Trigger accepted AND downstream outcome achieved"
    observation_window_days: 30
    data_sources: 
      - "Veeva (CRM)"
      - "IQVIA APLD"
    edge_cases: 
      - "trigger_not_generated"

edge_case_taxonomy:
  new_hcp:
    applies_to: "HCP Churn"
    description: "HCP with insufficient prior prescribing history (<3 scripts)"
    problem: "Cannot establish reliable baseline for churn detection"
    handling: "Label as INDETERMINATE, confidence = 0.50"
    exclusion_reason: "Insufficient prior activity (<1 TRx)"

  seasonal_effects:
    applies_to: "HCP Churn"
    description: "Temporary prescribing dips (Q4 holidays, summer slowdowns)"
    problem: "May misclassify temporary reduction as churn"
    handling: "Require 2 consecutive quarters for high-confidence churn label"
    detection: "Compare to same-period prior year"

  data_source_lag:
    applies_to: "All models"
    description: "Prediction made before data source lag period elapsed"
    problem: "Cannot query outcomes that haven't arrived yet"
    handling: "Defer labeling until lag_days + observation_window elapsed"
    lag_values:
      IQVIA_APLD: "12d"
      IQVIA_LAAD: "14d"
      HealthVerity: "7d"
      Komodo: "10d"
      Veeva: "0d"

  trigger_not_delivered:
    applies_to: 
      - "Script Conversion"
      - "NBA"
    description: "Trigger was generated but never delivered or viewed"
    problem: "Cannot attribute conversion to a trigger that wasn't seen"
    handling: "Label as EXCLUDED, track for trigger_acceptance analysis"
    exclusion_reason: "Trigger not delivered"

  multi_trigger:
    applies_to: "Script Conversion"
    description: "Multiple triggers fired within the same observation window"
    problem: "Unclear which trigger caused the conversion"
    handling: "Attribute to first trigger, set partial_attribution flag on subsequent"
    analysis: "Separate multi-touch attribution study"

  therapy_switch:
    applies_to: "Treatment Response"
    description: "Patient switched to different brand (not discontinuation)"
    problem: "Not a persistence failure, but a brand loss"
    handling: "Label as SWITCHED (separate category), exclude from non-persistence"
    detection: "Check for fills of competitor brands"

  clinical_discontinuation:
    applies_to: "Treatment Response"
    description: "MD-directed therapy stop (disease resolved, adverse event, contraindication)"
    problem: "Appropriate clinical decision, not a persistence failure"
    handling: "Label as APPROPRIATE_STOP, exclude from negative outcomes"
    detection: "Check for discontinuation_reason in treatment_events"

  market_disruption:
    applies_to: "Market Share Impact"
    description: "Major market event (competitor launch, recall, supply issue)"
    problem: "Confounds market share predictions with external factors"
    handling: "Label as CONFOUNDED, lower weight in drift detection"
    detection: "External event calendar, competitor intelligence"

database_objects:
  tables:
    ml_predictions:
      purpose: "Extended with ground truth columns"
      key_columns: 
        - "actual_outcome"
        - "outcome_recorded_at"
        - "truth_source"
        - "truth_confidence"
        - "outcome_label"
        - "exclusion_reason"
    
    ml_feedback_loop_config:
      purpose: "Configuration per model type"
      key_columns: 
        - "prediction_type"
        - "observation_window_days"
        - "min_observation_days"
        - "schedule_cron"
    
    ml_feedback_loop_runs:
      purpose: "Execution history"
      key_columns: 
        - "prediction_type"
        - "run_status"
        - "predictions_evaluated"
        - "predictions_labeled"

  functions:
    assign_truth_hcp_churn:
      purpose: "Label churn predictions"
      parameters: 
        - "observation_window_days"
        - "decline_threshold"
        - "min_prior_scripts"
        - "batch_size"
    
    assign_truth_script_conversion:
      purpose: "Label conversion predictions"
      parameters: 
        - "observation_window_days"
        - "batch_size"
    
    assign_truth_treatment_response:
      purpose: "Label persistence predictions"
      parameters: 
        - "observation_window_days"
        - "pdc_threshold"
        - "max_gap_days"
        - "batch_size"
    
    assign_truth_next_best_action:
      purpose: "Label NBA predictions"
      parameters: 
        - "observation_window_days"
        - "batch_size"
    
    assign_truth_market_share:
      purpose: "Label market share predictions"
      parameters: 
        - "observation_window_days"
        - "accuracy_threshold"
        - "batch_size"
    
    run_feedback_loop:
      purpose: "Master orchestrator"
      parameters: "prediction_type (optional, NULL = all)"
    
    get_pending_predictions_summary:
      purpose: "Check labeling backlog"
      parameters: null
    
    cleanup_feedback_loop_logs:
      purpose: "Purge old run logs"
      parameters: "retention_days"

  views:
    v_concept_drift_metrics:
      purpose: "Weekly drift analysis"
      key_metrics: 
        - "accuracy"
        - "calibration_error"
        - "brier_score"
        - "actual_positive_rate"
    
    v_model_performance_tracking:
      purpose: "Monthly performance summary"
      key_metrics: 
        - "total_labeled"
        - "true_positives"
        - "avg_days_to_truth"
    
    v_drift_alerts:
      purpose: "Threshold-based alerting"
      key_metrics: 
        - "accuracy_status"
        - "calibration_status"
        - "class_shift_status"

metrics_glossary:
  accuracy_over_time:
    definition: "Model accuracy calculated on rolling windows of labeled predictions"
    formula: "(TP + TN) / Total"
    alert_threshold: ">5% drop from 90-day baseline triggers ALERT"

  calibration_error:
    definition: "Difference between predicted probability and observed frequency"
    formula: "|avg(prediction_value) - avg(actual_outcome)|"
    ideal_value: 0
    alert_threshold: ">10% increase from baseline triggers ALERT"

  class_ratio_shift:
    definition: "Change in the proportion of positive outcomes over time"
    formula: "|current_positive_rate - baseline_positive_rate|"
    alert_threshold: ">15% shift triggers ALERT"
    interpretation: "May indicate concept drift OR external market changes"

  brier_score:
    definition: "Mean squared error of probabilistic predictions"
    formula: "(1/N) * sum(predicted_prob - actual_outcome)^2"
    range: "0-1 (lower is better)"
    usage: "Single metric combining calibration and discrimination"

  time_to_truth:
    definition: "Average days from prediction to outcome label availability"
    formula: "avg(outcome_recorded_at - prediction_timestamp)"
    usage: "Monitor labeling pipeline health"

  indeterminate_rate:
    definition: "Proportion of predictions that cannot be reliably labeled"
    formula: "count(INDETERMINATE) / count(total_evaluated)"
    alert_threshold: ">20% triggers review"

  exclusion_rate:
    definition: "Proportion of predictions excluded due to confounding"
    formula: "count(EXCLUDED) / count(total_evaluated)"
    usage: "Track data quality and edge case frequency"

nlv_query_patterns:
  - category: "Concept Drift Queries"
    intent: "DRIFT_ANALYSIS"
    agent: "drift_monitor"
    routes: 
      - "v_concept_drift_metrics"
      - "v_drift_alerts"
    examples:
      - "Is the churn model experiencing concept drift?"
      - "Show me accuracy trends for the trigger model"
      - "What's the calibration error for market share predictions?"
      - "Are there any drift alerts this week?"

  - category: "Feedback Loop Status Queries"
    intent: "SYSTEM_STATUS"
    agent: 
      - "health_score"
      - "observability"
    routes: 
      - "get_pending_predictions_summary()"
      - "ml_feedback_loop_runs"
    examples:
      - "How many predictions are pending labels?"
      - "When was the last feedback loop run?"
      - "What's the labeling backlog for treatment response?"
      - "Show me the feedback loop history"

  - category: "Ground Truth Queries"
    intent: "MODEL_PERFORMANCE"
    agent: "feedback_learner"
    routes: 
      - "v_model_performance_tracking"
      - "ml_predictions (labeled)"
    examples:
      - "What's the actual churn rate vs predicted?"
      - "Show me conversion truth by region"
      - "What percentage of churn predictions were accurate?"
      - "Compare predicted vs actual market share"

  - category: "Edge Case Queries"
    intent: "DATA_QUALITY"
    agent: 
      - "data_preparer"
      - "health_score"
    routes: 
      - "ml_predictions (exclusion_reason)"
      - "v_model_performance_tracking"
    examples:
      - "How many predictions were excluded?"
      - "What are the main exclusion reasons?"
      - "Show me indeterminate rate trends"
      - "Which edge cases are most common for churn?"

agent_integration:
  drift_monitor:
    tier: 3
    existing_capabilities: "Feature drift via PSI, KS test, chi-square, Jensen-Shannon"
    extended_capabilities: "Concept drift via accuracy_over_time, calibration_drift, class_ratio_shift"
    input: "v_concept_drift_metrics"
    output: "DriftAlert to orchestrator"

  feedback_learner:
    tier: 5
    input: "Labeled predictions with actual_outcome"
    processing: "Analyze prediction errors, identify systematic failures"
    output: "Retraining signals, experiment recommendations to ExperimentKnowledgeStore"

  experiment_monitor:
    tier: 3
    status: "New"
    purpose: "Orchestrate feedback loop execution"
    schedule: "Cron-based per model type"
    input: "ml_feedback_loop_config"
    output: "Ground truth labels in ml_predictions"

  health_score:
    tier: 3
    extended_capabilities: "Include feedback loop health in system health composite"
    metrics: 
      - "pending_backlog"
      - "run_success_rate"
      - "avg_time_to_truth"
      - "indeterminate_rate"

synonyms_and_aliases:
  actual_outcome:
    - "ground truth"
    - "true label"
    - "observed outcome"
    - "real outcome"
  observation_window:
    - "lookforward window"
    - "attribution window"
    - "outcome period"
  label_lag:
    - "outcome delay"
    - "truth lag"
    - "feedback delay"
  concept_drift:
    - "relationship drift"
    - "target drift"
    - "posterior drift"
  feature_drift:
    - "data drift"
    - "covariate shift"
    - "input drift"
  truth_confidence:
    - "label confidence"
    - "outcome reliability"
  feedback_loop:
    - "labeling pipeline"
    - "truth assignment"
    - "outcome collection"
  INDETERMINATE:
    - "unknown"
    - "uncertain"
    - "insufficient data"
  EXCLUDED:
    - "confounded"
    - "removed"
    - "filtered out"
  calibration_error:
    - "miscalibration"
    - "probability error"

version_history:
  - version: "3.2.0"
    date: "2025-12-23"
    changes: "Added feedback loop vocabulary extension"
  - version: "3.1.0"
    date: "2025-12-XX"
    changes: "Base vocabulary with 18-agent architecture"
  - version: "3.0.0"
    date: "2025-XX-XX"
    changes: "Initial V3 schema vocabulary"