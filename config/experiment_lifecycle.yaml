# ═══════════════════════════════════════════════════════════════════
# E2I Causal Analytics - Experiment Lifecycle Configuration
# File: experiment_lifecycle.yaml
# Version: V4.1
# Purpose: Define experiment state machine, interim analysis, and stopping rules
# Gap: Gap 13 - Experiment Lifecycle Workflow
# ═══════════════════════════════════════════════════════════════════

# ═══════════════════════════════════════════════════════════════════
# EXPERIMENT LIFECYCLE OVERVIEW
# ═══════════════════════════════════════════════════════════════════

lifecycle:
  description: |
    Experiments progress through a defined lifecycle from design → launch →
    monitoring → analysis → decision. This configuration defines state
    transitions, approval gates, and decision criteria.

  responsible_agent: "experiment_designer"

  integration:
    database_table: "ml_experiments"
    status_field: "experiment_status"
    state_history_table: "experiment_state_history"


# ═══════════════════════════════════════════════════════════════════
# EXPERIMENT STATES
# ═══════════════════════════════════════════════════════════════════

states:
  draft:
    description: "Experiment is being designed"
    allowed_actions:
      - edit_design
      - calculate_sample_size
      - simulate_power
      - request_review
    next_states: [in_review, cancelled]
    auto_transition: false

  in_review:
    description: "Experiment design under stakeholder review"
    allowed_actions:
      - approve
      - request_changes
      - reject
    next_states: [approved, draft, rejected]
    auto_transition: false
    review_required_from:
      - analytics_lead
      - brand_lead
    sla_hours: 48  # Must be reviewed within 48 hours

  approved:
    description: "Experiment design approved, ready for implementation"
    allowed_actions:
      - implement
      - assign_treatment
      - schedule_launch
    next_states: [scheduled, cancelled]
    auto_transition: false

  scheduled:
    description: "Experiment scheduled to launch at specific date/time"
    allowed_actions:
      - launch_now
      - reschedule
      - cancel
    next_states: [running, cancelled]
    auto_transition: true  # Auto-launch at scheduled time
    pre_launch_checks:
      - verify_randomization
      - check_data_pipelines
      - validate_treatment_assignment
      - confirm_tracking_enabled

  running:
    description: "Experiment is live and collecting data"
    allowed_actions:
      - monitor
      - run_interim_analysis
      - stop_for_success
      - stop_for_futility
      - stop_for_harm
      - pause
    next_states: [paused, stopped_success, stopped_futility, stopped_harm, completed]
    auto_transition: true  # Can auto-stop based on stopping rules
    monitoring_frequency: "daily"
    interim_analysis_enabled: true

  paused:
    description: "Experiment temporarily paused"
    allowed_actions:
      - resume
      - stop
      - investigate_issue
    next_states: [running, stopped_other]
    auto_transition: false
    max_pause_duration_days: 14  # Auto-stop if paused > 14 days

  stopped_success:
    description: "Experiment stopped early due to significant positive effect"
    allowed_actions:
      - analyze_final_results
      - document_decision
    next_states: [analyzed]
    auto_transition: true
    stopping_reason: "early_success"

  stopped_futility:
    description: "Experiment stopped early due to futility (unlikely to reach significance)"
    allowed_actions:
      - analyze_final_results
      - document_decision
    next_states: [analyzed]
    auto_transition: true
    stopping_reason: "futility"

  stopped_harm:
    description: "Experiment stopped early due to adverse effects"
    allowed_actions:
      - analyze_final_results
      - document_decision
      - create_incident_report
    next_states: [analyzed]
    auto_transition: true
    stopping_reason: "safety_concern"
    alert_severity: "critical"

  stopped_other:
    description: "Experiment stopped for other reasons (business decision, technical issue)"
    allowed_actions:
      - analyze_final_results
      - document_decision
    next_states: [analyzed]
    auto_transition: false
    stopping_reason: "other"

  completed:
    description: "Experiment ran to planned completion"
    allowed_actions:
      - analyze_final_results
      - document_decision
    next_states: [analyzed]
    auto_transition: true

  analyzed:
    description: "Final analysis completed, decision pending"
    allowed_actions:
      - make_decision
      - request_additional_analysis
    next_states: [decided]
    auto_transition: false

  decided:
    description: "Final decision made (implement winner, no change, or further testing)"
    allowed_actions:
      - document_learnings
      - implement_winner
      - archive
    next_states: [archived]
    auto_transition: false
    decision_options:
      - implement_treatment
      - implement_control
      - no_change
      - run_followup_experiment

  rejected:
    description: "Experiment design rejected during review"
    allowed_actions:
      - archive
    next_states: [archived]
    auto_transition: false

  cancelled:
    description: "Experiment cancelled before launch"
    allowed_actions:
      - archive
    next_states: [archived]
    auto_transition: false

  archived:
    description: "Experiment archived (terminal state)"
    allowed_actions:
      - view_historical_data
    next_states: []
    auto_transition: false


# ═══════════════════════════════════════════════════════════════════
# STATE TRANSITIONS
# ═══════════════════════════════════════════════════════════════════

transitions:
  draft_to_in_review:
    trigger: "manual"
    action: "request_review"
    required_fields:
      - experiment_name
      - hypothesis
      - treatment_definition
      - control_definition
      - primary_metric
      - sample_size
      - planned_duration
      - randomization_unit
    validations:
      - check_sample_size_adequate
      - check_metric_trackable
      - check_no_overlapping_experiments
    notifications:
      - notify_reviewers

  in_review_to_approved:
    trigger: "manual"
    action: "approve"
    required_approvals: 2  # Need 2 approvals
    validations:
      - all_required_fields_present
      - statistical_power_adequate
    notifications:
      - notify_experiment_owner
      - notify_implementation_team

  in_review_to_draft:
    trigger: "manual"
    action: "request_changes"
    required_fields:
      - change_request_notes
    notifications:
      - notify_experiment_owner

  approved_to_scheduled:
    trigger: "manual"
    action: "schedule_launch"
    required_fields:
      - launch_date
      - launch_time
      - timezone
    validations:
      - launch_date_in_future
      - no_conflicting_experiments
      - implementation_confirmed
    notifications:
      - notify_stakeholders

  scheduled_to_running:
    trigger: "automatic"  # At scheduled time
    action: "launch"
    pre_flight_checks:
      - verify_randomization_working
      - check_tracking_enabled
      - validate_treatment_assignment
      - confirm_data_pipeline_ready
    on_failure: "pause"
    notifications:
      - notify_stakeholders
      - alert_on_launch_failure

  running_to_stopped_success:
    trigger: "automatic"  # Based on stopping rules
    action: "stop_for_success"
    conditions:
      - early_stopping_criteria_met
      - minimum_sample_size_reached
    validations:
      - verify_significance
      - check_effect_size
    notifications:
      - notify_stakeholders
      - create_success_report

  running_to_stopped_futility:
    trigger: "automatic"
    action: "stop_for_futility"
    conditions:
      - futility_criteria_met
      - minimum_runtime_elapsed
    notifications:
      - notify_stakeholders

  running_to_stopped_harm:
    trigger: "automatic"
    action: "stop_for_harm"
    conditions:
      - harm_detected
    immediate: true  # Stop immediately
    notifications:
      - alert_critical
      - notify_safety_team
      - create_incident

  running_to_paused:
    trigger: "manual"
    action: "pause"
    required_fields:
      - pause_reason
    notifications:
      - notify_stakeholders

  paused_to_running:
    trigger: "manual"
    action: "resume"
    validations:
      - pause_issue_resolved
      - data_quality_check
    notifications:
      - notify_stakeholders

  running_to_completed:
    trigger: "automatic"
    action: "complete"
    conditions:
      - planned_duration_reached
      - target_sample_size_reached
    notifications:
      - notify_stakeholders
      - request_final_analysis

  completed_to_analyzed:
    trigger: "automatic"
    action: "analyze"
    perform:
      - run_statistical_tests
      - calculate_confidence_intervals
      - perform_subgroup_analysis
      - generate_final_report
    notifications:
      - notify_decision_makers

  analyzed_to_decided:
    trigger: "manual"
    action: "make_decision"
    required_fields:
      - decision
      - decision_rationale
    notifications:
      - notify_stakeholders
      - update_roadmap


# ═══════════════════════════════════════════════════════════════════
# INTERIM ANALYSIS CONFIGURATION
# ═══════════════════════════════════════════════════════════════════

interim_analysis:
  description: |
    Interim analyses allow monitoring experiment progress and making
    early stopping decisions while controlling Type I error inflation.

  enabled: true

  # When to perform interim analyses
  schedule:
    method: "information_fraction"  # or "time_based"

    # Information fraction method: analyze at 25%, 50%, 75%, 100% of planned sample
    information_fractions: [0.25, 0.50, 0.75, 1.00]

    # Time-based method: analyze at fixed intervals
    time_based_intervals:
      frequency: "weekly"
      start_after_days: 7  # First analysis after 1 week

  # Minimum requirements before first interim analysis
  minimum_requirements:
    min_sample_size: 100  # Per arm
    min_runtime_days: 7
    min_events: 50  # For event-based metrics

  # Multiple testing correction
  multiple_testing_correction:
    method: "lan_demets_obrien_fleming"  # Conservative alpha spending

    alternatives:
      - "pocock"  # Less conservative, more likely to stop early
      - "haybittle_peto"  # Very conservative
      - "bonferroni"  # Simple but overly conservative

    # O'Brien-Fleming alpha spending function
    obrien_fleming:
      overall_alpha: 0.05
      # Adjusted alphas at each look (4 looks)
      adjusted_alphas: [0.0005, 0.014, 0.023, 0.041]

  # What to calculate in interim analysis
  calculations:
    - effect_size_estimate
    - confidence_intervals
    - p_values
    - conditional_power
    - predictive_probability_of_success
    - futility_index

  # Output interim report
  report:
    generate: true
    include_stopping_recommendation: true
    mask_results: false  # Set true for blinded reviews
    notify: ["experiment_owner", "analytics_lead"]


# ═══════════════════════════════════════════════════════════════════
# EARLY STOPPING RULES
# ═══════════════════════════════════════════════════════════════════

early_stopping:
  description: |
    Rules for stopping experiments early based on efficacy, futility, or harm.

  # Efficacy stopping (early success)
  efficacy:
    enabled: true

    criteria:
      statistical_significance:
        # Must meet adjusted alpha threshold from sequential design
        p_value_threshold: "adjusted_alpha"
        two_sided: true

      practical_significance:
        # Effect size must be meaningful, not just statistically significant
        minimum_effect_size:
          relative_lift: 0.10  # At least +10% improvement
          absolute_lift: 100  # At least +100 units (e.g., TRx)

      consistency:
        # Effect must be consistent across key subgroups
        minimum_consistency_score: 0.70  # From Gap 12 confidence logic

    # Additional safeguards
    safeguards:
      minimum_runtime_days: 14  # Don't stop before 2 weeks
      minimum_sample_per_arm: 100
      require_positive_in_all_subgroups: false
      require_approval: true  # Manual approval even if criteria met

    # Stopping recommendation
    recommendation:
      confidence_threshold: 0.90  # Stop if 90%+ confident in success
      expected_benefit: "Calculate incremental value using ROI methodology"


  # Futility stopping (unlikely to succeed)
  futility:
    enabled: true

    criteria:
      conditional_power:
        # Probability of achieving significance if experiment continues
        threshold: 0.20  # Stop if <20% chance of success

      predictive_probability:
        # Bayesian predictive probability of success
        threshold: 0.10  # Stop if <10% posterior probability

      trend_analysis:
        # If effect trending in wrong direction
        check_trend: true
        consecutive_negative_looks: 2  # Stop if 2 consecutive interim analyses show negative trend

    # Safeguards
    safeguards:
      minimum_runtime_days: 21  # Run at least 3 weeks before futility stop
      minimum_sample_per_arm: 150
      require_approval: false  # Can auto-stop for futility

    # Stopping recommendation
    recommendation:
      confidence_threshold: 0.80  # Stop if 80%+ confident of futility


  # Harm stopping (adverse effects)
  harm:
    enabled: true

    criteria:
      adverse_metric_deterioration:
        # Stop if key metrics significantly worse in treatment
        metrics:
          - name: "hcp_opt_out_rate"
            threshold: 0.15  # +15% increase in opt-outs
            p_value: 0.05

          - name: "patient_adverse_events"
            threshold: 0.10  # +10% increase in adverse events
            p_value: 0.01

          - name: "data_quality_score"
            threshold: -10  # -10 point drop in quality
            p_value: 0.05

      business_metric_harm:
        # Stop if critical business metrics harmed
        metrics:
          - name: "trx_volume"
            threshold: -0.05  # -5% decline
            p_value: 0.05

          - name: "market_share"
            threshold: -0.02  # -2% decline
            p_value: 0.10

    # Safeguards
    safeguards:
      minimum_sample_per_arm: 50  # Can stop early for harm
      confirmation_required: true  # Require 2 consecutive checks
      immediate_stop: true  # Stop immediately if harm detected

    # Stopping recommendation
    recommendation:
      confidence_threshold: 0.95  # High confidence required
      immediate_alert: true
      severity: "critical"


# ═══════════════════════════════════════════════════════════════════
# SAMPLE SIZE RE-ESTIMATION
# ═══════════════════════════════════════════════════════════════════

sample_size_reestimation:
  description: |
    Adjust sample size mid-experiment based on observed variance and effect size.

  enabled: true

  # When to perform re-estimation
  timing:
    at_information_fraction: 0.50  # Re-estimate at 50% of planned sample
    require_blinding: false  # Can use unblinded data

  # Re-estimation method
  method: "variance_adjustment"  # Adjust for observed variance

  # Adjustment rules
  rules:
    increase_sample_size:
      condition: "observed_variance > assumed_variance * 1.25"
      max_increase_factor: 1.50  # Can increase by up to 50%
      require_approval: true

    decrease_sample_size:
      condition: "observed_effect_size > assumed_effect_size * 1.50"
      max_decrease_factor: 0.75  # Can decrease by up to 25%
      require_approval: true

    no_change:
      condition: "observed parameters within expected range"

  # Constraints
  constraints:
    absolute_minimum_per_arm: 100
    absolute_maximum_per_arm: 5000
    max_duration_extension_days: 60


# ═══════════════════════════════════════════════════════════════════
# EXPERIMENT DESIGN PARAMETERS
# ═══════════════════════════════════════════════════════════════════

design_parameters:
  # Default statistical parameters for sample size calculation

  statistical_power:
    default: 0.80  # 80% power
    minimum: 0.70
    recommended: 0.85

  significance_level:
    default: 0.05  # α = 5%
    one_sided: 0.025
    two_sided: 0.05

  effect_size:
    # Minimum detectable effect (MDE)
    relative:
      default: 0.10  # 10% relative improvement
      minimum: 0.05
      maximum: 0.50

    absolute:
      # Depends on metric - defined per metric type
      trx_volume: 100  # +100 TRx
      hcp_engagement_rate: 0.05  # +5pp
      market_share: 0.02  # +2pp

  randomization:
    unit: "hcp"  # or "patient", "territory", "time_period"
    method: "simple_random"

    alternatives:
      - "stratified_random"  # Stratify by tier, region
      - "cluster_random"  # Randomize by territory
      - "matched_pairs"  # Pair similar units

    # Treatment allocation ratio
    allocation_ratio:
      default: "1:1"  # Equal allocation
      alternatives:
        - "2:1"  # 2 treatment : 1 control (faster learning on treatment)
        - "3:1"  # 3 treatment : 1 control (aggressive)

  # Experiment duration
  duration:
    minimum_days: 14
    recommended_days: 42  # 6 weeks
    maximum_days: 180  # 6 months

    # Calculate based on expected event rate
    calculate_from_events:
      enabled: true
      events_per_unit_per_day: 0.05  # Expected event rate
      target_events_per_arm: 200


# ═══════════════════════════════════════════════════════════════════
# EXPERIMENT MONITORING
# ═══════════════════════════════════════════════════════════════════

monitoring:
  # Real-time monitoring of running experiments

  enabled: true
  frequency: "daily"

  checks:
    balance_checks:
      # Verify treatment/control balance on covariates
      enabled: true
      frequency: "weekly"
      covariates:
        - hcp_tier
        - specialty
        - region
        - baseline_trx_volume
      max_imbalance_threshold: 0.10  # 10% difference

    sample_ratio_mismatch:
      # Check if treatment/control allocation ratio as expected
      enabled: true
      frequency: "daily"
      expected_ratio: [1, 1]  # 1:1
      tolerance: 0.05  # ±5%

    data_quality:
      # Monitor data quality during experiment
      enabled: true
      frequency: "daily"
      metrics:
        - missing_rate
        - duplicate_rate
        - outlier_rate
      alert_thresholds:
        missing_rate: 0.10
        duplicate_rate: 0.05
        outlier_rate: 0.05

    treatment_compliance:
      # Check if treatment is being delivered as intended
      enabled: true
      frequency: "daily"
      minimum_compliance_rate: 0.90  # 90%

    metric_tracking:
      # Verify primary and secondary metrics being tracked
      enabled: true
      frequency: "daily"
      metrics: ["primary_metric", "secondary_metrics", "guardrail_metrics"]

  # Alerts for monitoring issues
  alerts:
    balance_violation:
      severity: "warning"
      category: "experiment_monitoring"

    srm_detected:
      severity: "urgent"
      category: "experiment_monitoring"

    low_compliance:
      severity: "warning"
      category: "experiment_monitoring"

    missing_data:
      severity: "warning"
      category: "data_quality"


# ═══════════════════════════════════════════════════════════════════
# FINAL ANALYSIS
# ═══════════════════════════════════════════════════════════════════

final_analysis:
  # Analysis performed when experiment reaches terminal state

  statistical_tests:
    primary_metric:
      test: "two_sample_t_test"  # or "mann_whitney", "chi_square"
      sided: "two_sided"
      alpha: 0.05
      bonferroni_correction: false

    secondary_metrics:
      test: "two_sample_t_test"
      sided: "two_sided"
      alpha: 0.01  # More stringent
      bonferroni_correction: true  # Correct for multiple comparisons

  effect_size_estimation:
    method: "difference_in_means"  # or "cohens_d", "relative_uplift"
    confidence_level: 0.95
    bootstrap_samples: 1000

  subgroup_analysis:
    enabled: true
    subgroups:
      - hcp_tier
      - region
      - specialty
      - baseline_trx_volume_quartile

    # Multiple testing correction for subgroups
    correction_method: "benjamini_hochberg"  # FDR control

  causal_inference:
    # Apply causal inference methods (Gap 3)
    enabled: true
    method: "difference_in_differences"  # or "propensity_score_matching"

    validation:
      run_dowhy_refutation: true
      confidence_calculation: true  # Use Gap 12 confidence logic

  heterogeneous_effects:
    # Check for heterogeneous treatment effects (Gap 5)
    enabled: true
    method: "causal_forest"
    report_uplift_by_segment: true

  roi_calculation:
    # Calculate ROI (Gap 9)
    enabled: true
    include_implementation_cost: true
    include_opportunity_cost: true
    confidence_intervals: true


# ═══════════════════════════════════════════════════════════════════
# DECISION FRAMEWORK
# ═══════════════════════════════════════════════════════════════════

decision_framework:
  # Framework for making go/no-go decisions post-experiment

  criteria:
    statistical_significance:
      weight: 0.25
      threshold: 0.05  # p-value

    practical_significance:
      weight: 0.25
      threshold: 0.10  # 10% relative lift

    confidence_score:
      weight: 0.20
      threshold: 0.75  # High confidence (Gap 12)

    roi:
      weight: 0.20
      threshold: 3.0  # 3x ROI (Gap 9)

    business_alignment:
      weight: 0.10
      threshold: "qualitative"  # Stakeholder input

  # Decision thresholds
  thresholds:
    strong_go:
      score: 0.85  # Weighted score ≥ 85%
      recommendation: "Implement treatment immediately"

    go:
      score: 0.70  # 70-84%
      recommendation: "Implement treatment with monitoring"

    consider:
      score: 0.50  # 50-69%
      recommendation: "Run follow-up experiment or pilot"

    no_go:
      score: 0.50  # < 50%
      recommendation: "Do not implement; iterate on treatment"

  # Post-decision actions
  post_decision:
    implement_treatment:
      actions:
        - "Rollout treatment to full population"
        - "Update operational procedures"
        - "Monitor impact in production"
        - "Document learnings"

    no_change:
      actions:
        - "Keep control/baseline"
        - "Document why treatment didn't work"
        - "Generate hypotheses for next iteration"

    run_followup:
      actions:
        - "Design follow-up experiment"
        - "Refine treatment based on learnings"
        - "Adjust target population"


# ═══════════════════════════════════════════════════════════════════
# DOCUMENTATION AND REPORTING
# ═══════════════════════════════════════════════════════════════════

documentation:
  # Required documentation at each stage

  design_doc:
    required_sections:
      - hypothesis
      - treatment_description
      - control_description
      - primary_metric
      - secondary_metrics
      - guardrail_metrics
      - sample_size_calculation
      - power_analysis
      - randomization_plan
      - analysis_plan

    template_url: "https://docs.internal/templates/experiment-design"

  analysis_report:
    required_sections:
      - executive_summary
      - statistical_results
      - effect_size_estimates
      - subgroup_analysis
      - causal_validation_results
      - confidence_assessment
      - roi_calculation
      - limitations
      - recommendation

    template_url: "https://docs.internal/templates/experiment-analysis"

  decision_memo:
    required_sections:
      - decision
      - rationale
      - implementation_plan
      - success_metrics
      - rollback_plan

    template_url: "https://docs.internal/templates/experiment-decision"


# ═══════════════════════════════════════════════════════════════════
# INTEGRATION WITH OTHER SYSTEMS
# ═══════════════════════════════════════════════════════════════════

integration:
  # Integration points with other E2I components

  causal_validation:
    # Gap 3: Run DoWhy refutation tests
    enabled: true
    run_on: "final_analysis"
    store_results_in: "causal_validations table"

  confidence_scoring:
    # Gap 12: Calculate confidence scores
    enabled: true
    run_on: "final_analysis"
    store_results_in: "causal_impacts table"

  roi_calculation:
    # Gap 9: Calculate ROI
    enabled: true
    run_on: "final_analysis"
    store_results_in: "gap_analysis_results table"

  alert_system:
    # Gap 11: Trigger alerts
    enabled: true
    alert_on:
      - experiment_stopped_harm
      - srm_detected
      - low_compliance
      - data_quality_issue

  agent_activities:
    # Log all experiment lifecycle events
    enabled: true
    log_state_transitions: true
    log_interim_analyses: true
    log_stopping_decisions: true


# ═══════════════════════════════════════════════════════════════════
# TESTING AND SIMULATION
# ═══════════════════════════════════════════════════════════════════

testing:
  # Test experiment workflows

  simulation:
    enabled: true

    # Simulate experiment outcomes for power analysis
    simulate_scenarios:
      - scenario: "null_effect"
        true_effect: 0.0
        iterations: 1000
        check: "Type I error rate ≈ 0.05"

      - scenario: "small_effect"
        true_effect: 0.05
        iterations: 1000
        check: "Power ≈ designed power"

      - scenario: "large_effect"
        true_effect: 0.20
        iterations: 1000
        check: "Early stopping rate"

  validation:
    # Validate state machine
    validate_transitions: true
    check_terminal_states_reachable: true
    check_no_invalid_transitions: true


# ═══════════════════════════════════════════════════════════════════
# GOVERNANCE
# ═══════════════════════════════════════════════════════════════════

governance:
  owner: "VP Analytics & Data Science"

  approvers:
    experiment_design: ["Analytics Lead", "Brand Lead"]
    early_stopping: ["Analytics Lead"]
    final_decision: ["Brand Lead", "CMO"]

  review_cadence:
    design_reviews: "weekly"
    running_experiments_review: "weekly"
    post_mortem: "within 2 weeks of completion"


# ═══════════════════════════════════════════════════════════════════
# VERSION HISTORY
# ═══════════════════════════════════════════════════════════════════

version: "4.1"
last_updated: "2025-12-15"
changelog:
  - version: "4.1"
    date: "2025-12-15"
    changes:
      - "Initial creation for Gap 13 resolution"
      - "Integrated with causal validation system (Gap 3)"
      - "Integrated with confidence scoring (Gap 12)"
      - "Integrated with ROI methodology (Gap 9)"
      - "Integrated with alert system (Gap 11)"
      - "Added heterogeneous effects analysis (Gap 5)"
